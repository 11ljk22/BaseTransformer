{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983ef5df",
   "metadata": {},
   "source": [
    "# Transformer模型变体：从序列任务到文本生成的架构调整\n",
    "Transformer模型的经典设计围绕**序列到序列（Sequence-to-Sequence, Seq2Seq）任务**展开，例如机器翻译、文本摘要等。为实现“输入序列→输出序列”的映射，它主要依赖两个核心块：\n",
    "1. **编码器-解码器（Encoder-Decoder）架构**：编码器负责对输入源序列进行特征提取与语义编码，解码器则基于编码结果生成目标序列；\n",
    "2. **解码器中的交叉注意力（Cross-Attention）机制**：这一关键组件能让解码器在生成每一步时，精准关联源序列的相关信息，确保输出与输入的语义一致性。\n",
    "\n",
    "\n",
    "如果我们要是实现的是文本生成而并非文本翻译，我们可以有如下调整：\n",
    "- **保留Transformer编码器核心**：复用编码器的多头自注意力（Multi-Head Self-Attention）、前馈神经网络（Feed-Forward Network）等模块，确保对文本序列的语义理解与特征建模能力；\n",
    "- **移除交叉注意力机制**：由于文本生成任务更侧重“基于历史生成内容延续序列”，而非“关联外部源序列”，交叉注意力不再是必需组件，移除后可简化模型结构、降低计算成本。\n",
    "\n",
    "通过上述调整，即可得到一个专为文本生成优化的Transformer变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0acbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175d301",
   "metadata": {},
   "source": [
    "# 位置编码与嵌入\n",
    "\n",
    "我们同样需要对输入序列进行词嵌入和位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c72f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPositionEncode(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout减少过拟合\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # x_1:(batch_size,seq_len,d_model)\n",
    "        x_1 = self.embedding(input_tensor)\n",
    "        seq_len = input_tensor.shape[1]\n",
    "\n",
    "        # 创建位置编码(正余弦)\n",
    "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(\n",
    "            1\n",
    "        )  # unsqueeze(1)添加批次维度\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
    "            * (\n",
    "                -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
    "                / self.d_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加位置编码\n",
    "        x_2 = pos_encoding.unsqueeze(0)\n",
    "        return self.dropout(x_1 + x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785d6d1",
   "metadata": {},
   "source": [
    "# 多头注意力\n",
    "使用带掩码的多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model: int, heads: int, dropout: float = 0, mask: bool = False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 变换回 d_model\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.head_dim = d_model // heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        batch_size, seq_len_q = query.size(0), query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "        # 线性投影,分割多头\n",
    "        # (batch_size,heads,seq_len_q,head_dim)\n",
    "        q = (\n",
    "            self.W_q(query)\n",
    "            .view(batch_size, seq_len_q, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        k = (\n",
    "            self.W_k(key)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        v = (\n",
    "            self.W_v(value)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # 计算注意力分数\n",
    "        # scores:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        scores = q @ k.transpose(-2, -1)\n",
    "        # 因果掩码,防止模型看见未来的信息\n",
    "        if self.mask:\n",
    "            mask_matrix = torch.triu(\n",
    "                torch.full((seq_len_q, seq_len_k), float(\"-inf\")), diagonal=1\n",
    "            ).to(query.device)\n",
    "            scores = scores + mask_matrix\n",
    "        # 掩蔽字符<pad>,因为它无意义\n",
    "        if key_padding_mask is not None:\n",
    "            # 确保key_padding_mask是布尔类型\n",
    "            if key_padding_mask.dtype != torch.bool:\n",
    "                key_padding_mask = key_padding_mask.bool()\n",
    "\n",
    "            # 原始形状: (batch_size, seq_len_k)\n",
    "            # 目标形状: (batch_size, 1, 1, seq_len_k) ,这样可以广播到所有头和query位置\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "            # 应用掩码\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "        # 缩放并应用softmax\n",
    "        attention = nn.Softmax(dim=-1)(\n",
    "            scores / torch.sqrt(torch.tensor(self.head_dim, device=query.device))\n",
    "        )\n",
    "        # attention:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        attention = self.dropout(attention)\n",
    "        # 加权和\n",
    "        # out:(batch_size,heads,seq_len_q,head_dim)\n",
    "        out = attention @ v\n",
    "        # 拼接多头\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len_q, self.heads * self.head_dim)\n",
    "        )\n",
    "        # (batch_size,seq_len_q,d_model)\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9bb8a",
   "metadata": {},
   "source": [
    "# 解码器层\n",
    "\n",
    "从原来的解码器层中剔除了交叉注意力，仅仅保留带掩码的自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            d_model, heads, dropout, mask=True\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        # 自注意力子层\n",
    "        residual = x\n",
    "        x = self.multi_head_attention(x, x, x, key_padding_mask)\n",
    "        x = self.layer_norm_1(residual + self.dropout(x))\n",
    "\n",
    "        # 前馈子层\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.layer_norm_2(residual + self.dropout(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextGenerate(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, num_layers=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding_pos_encode = EmbeddingPositionEncode(\n",
    "            d_model, dropout, vocab_size\n",
    "        )\n",
    "\n",
    "        # 堆叠多层解码器\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        x = self.embedding_pos_encode(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, key_padding_mask)\n",
    "\n",
    "        return self.final_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26292e",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "训练步骤基本保持不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from LyricsDataset import LyricsDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 24\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "dataloader = DataLoader(dataset, batch_size)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "\n",
    "padding_idx = dataset.token_to_index[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "total_steps = int(epochs * (len(dataset) / batch_size))\n",
    "warmup_steps = max(1, int(total_steps * 0.4))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda step: (\n",
    "        512 ** (-0.5)  # 模型维度的平方根倒数\n",
    "        * min(\n",
    "            (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
    "            (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "print(f\"total_steps:{total_steps},warmup_steps:{warmup_steps}\")\n",
    "\n",
    "model.train().to(device)\n",
    "\n",
    "error = []\n",
    "path = None\n",
    "start_epoch = 0\n",
    "# start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path)\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    total_loss = 0\n",
    "    batch = 1\n",
    "    for src, tgt in dataloader:\n",
    "        print(f\"batch {batch}, {batch*batch_size}/{len(dataset)}\")\n",
    "        batch += 1\n",
    "        src_key_padding_mask = src == padding_idx\n",
    "        src, tgt, src_key_padding_mask = (\n",
    "            src.to(device),\n",
    "            tgt.to(device),\n",
    "            src_key_padding_mask.to(device),\n",
    "        )\n",
    "        pred = model(src, src_key_padding_mask)\n",
    "        loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    error.append(avg_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f}\"\n",
    "        )\n",
    "    torch.save(\n",
    "        model.state_dict(), f\"model_epoch_{epoch+1}.pth\"\n",
    "    )  # 由于是2w4的小样本数据集，我们可以不使用检查点，直接保存模型\n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    #     path_to_save = None\n",
    "    #     save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path_to_save)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af985fac",
   "metadata": {},
   "source": [
    "# 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    text: str,\n",
    "    model: nn.Module,\n",
    "    max_length: int,\n",
    "    separator: str,\n",
    "    device: str,\n",
    "    to_index: dict,\n",
    "    to_token: list,\n",
    "    tempreture: float = 0.75,  # 温度越小生成越确定（重复度高），值越大越多样（可能出现不合理内容）\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index[\"<bos>\"]] + [\n",
    "                to_index[char] for char in splitted_text\n",
    "            ]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                pred = model(tensor_text, None)[:, -1, :] / tempreture  # 应用温度\n",
    "                # 概率采样预测\n",
    "                proba = nn.Softmax(dim=-1)(pred)\n",
    "                dist = torch.distributions.Categorical(proba)\n",
    "                next_id = dist.sample()\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "        separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152971f1",
   "metadata": {},
   "source": [
    "# 使用训练好的模型\n",
    "\n",
    "该模型使用lyrics.csv数据集在下面代码的配置下使用3090 GPU训练得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LyricsDataset import LyricsDataset\n",
    "\n",
    "batch_size = 24\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"../3090_trained/model_epoch_200.pth\"))  # 加载模型\n",
    "text = \"晚风/我们\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    50,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    tempreture=0.95,\n",
    ")\n",
    "\n",
    "\n",
    "def wrap_text(text, width=60):\n",
    "    return \"\\n\".join([text[i : i + width] for i in range(0, len(text), width)])\n",
    "\n",
    "\n",
    "print(wrap_text(generated_lyrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
