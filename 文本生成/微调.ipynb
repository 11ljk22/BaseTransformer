{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3c5c4a",
   "metadata": {},
   "source": [
    "# 微调\n",
    "我们目前总是自己定义一个模型从头开始训练，但是这种做法比较耗费时间，且适用于数据集较大的情况下。因此我们可以采用一种更快速的方式，也就是在别人训练好的模型的基础上微调参数，让该模型快速适配我们的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f9cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:06:03.929019Z",
     "start_time": "2025-09-14T12:05:58.303927Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Model, AutoTokenizer  # 我们使用预训练的gpt2模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94b0b525614654",
   "metadata": {},
   "source": [
    "# 使用tokenizer的LyricsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b1b8cce33dcf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:06:03.934628Z",
     "start_time": "2025-09-14T12:06:03.929019Z"
    }
   },
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path: str,\n",
    "            separator: str = \"，\",\n",
    "            nrows: int = 300,\n",
    "            batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.separator = separator\n",
    "        self.nrows = nrows\n",
    "        self.batch_size = batch_size\n",
    "        # 使用预训练的 uer/gpt2-chinese-cluecorpussmall 的tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            r'D:\\cache\\models--uer--gpt2-chinese-cluecorpussmall\\snapshots\\c2c0249d8a2731f269414cc3b22dff021f8e07a3')\n",
    "        # 添加特殊标记\n",
    "        self.bos_token = '<bos>'\n",
    "        self.eos_token = '<eos>'\n",
    "        self.pad_token = '<pad>'\n",
    "        special_tokens = {'bos_token': self.bos_token,\n",
    "                          'eos_token': self.eos_token,\n",
    "                          'pad_token': self.pad_token}\n",
    "        # 添加我们自定义的三个token\n",
    "        self.num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "        # 计算更新之后的词表大小\n",
    "        self.vocab_size = self.tokenizer.vocab_size + self.num_added\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        self.data = self.read_file(file_path)\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        if self.nrows >= 0:\n",
    "            df = pd.read_csv(file_path, nrows=self.nrows, encoding='utf-8')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        # 返回原始数据\n",
    "        return df.values.reshape(-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 我们使用collate_fn将批次中的所有样本补充到同一个长度,因此这个函数不进行填充处理\n",
    "        src = self.data[index][:-1]\n",
    "        tgt = self.data[index][1:]\n",
    "\n",
    "        src = self.tokenizer(src, return_tensors='pt', add_special_tokens=False)['input_ids'].reshape(-1).tolist()\n",
    "        tgt = self.tokenizer(tgt, return_tensors='pt', add_special_tokens=False)['input_ids'].reshape(-1).tolist()\n",
    "        # 为src,tgt添加<bos>,<eos>\n",
    "        src = [self.bos_token_id] + src + [self.eos_token_id]\n",
    "        tgt = [self.bos_token_id] + tgt + [self.eos_token_id]\n",
    "\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"动态批次长度，这个函数可被指定为DataLoader数据加载器的collate_fn参数\"\"\"\n",
    "        batch_max_length = max(max(len(src), len(tgt)) for src, tgt in batch)\n",
    "        batch_src, batch_tgt = [], []\n",
    "        pad_idx = self.pad_token_id\n",
    "        for src, tgt in batch:\n",
    "            batch_src.append(pad(src, (0, batch_max_length - len(src)), value=pad_idx))\n",
    "            batch_tgt.append(pad(tgt, (0, batch_max_length - len(tgt)), value=pad_idx))\n",
    "        return torch.stack(batch_src), torch.stack(batch_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057bf5e",
   "metadata": {},
   "source": [
    "# 增加自定义语言模型头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb17f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:06:05.443237Z",
     "start_time": "2025-09-14T12:06:05.438726Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Lyrics(nn.Module):\n",
    "    def __init__(self, vocab_size, num_added_tokens):\n",
    "        super().__init__()\n",
    "        # 使用预训练的 uer/gpt2-chinese-cluecorpussmall 模型\n",
    "        # 为了加载方便，这里我使用下载好的本地模型,如果不使用本地模型，可将参数替换为uer/gpt2-chinese-cluecorpussmall\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\n",
    "            r\"D:\\cache\\models--uer--gpt2-chinese-cluecorpussmall\\snapshots\\c2c0249d8a2731f269414cc3b22dff021f8e07a3\")\n",
    "        # 因为我们手动添加了<bos>,<eos>,<pad>三个特殊字符，因此我们需要将这三个词加入到词嵌入层中\n",
    "        original_embedding = self.gpt2.wte.weight  # 获取原来的词嵌入层\n",
    "        # 创建新增的三个词的词嵌入参数，使用原词嵌入的均值，标准差，形状为(3,768),768是gpt2的模型维度\n",
    "        new_embeddings = torch.normal(mean=original_embedding.mean().item(),\n",
    "                                      std=original_embedding.std().item(),\n",
    "                                      size=(num_added_tokens, 768),\n",
    "                                      device=original_embedding.device)\n",
    "        # 将新增的词嵌入矩阵连接到原词嵌入矩阵\n",
    "        extended_embedding = torch.cat([original_embedding, new_embeddings], dim=0)\n",
    "        # 更新gpt2模型的词嵌入矩阵\n",
    "        self.gpt2.wte.weight = nn.Parameter(extended_embedding)\n",
    "        # 更新gpt2模型的 vocab_size 大小\n",
    "        self.gpt2.config.vocab_size = vocab_size\n",
    "\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            # 我们保留原词嵌入矩阵的值，关闭它们的梯度计算，我们只计算我们添加的三个词的梯度\n",
    "            if name == 'wte.weight':\n",
    "                param.requires_grad = False\n",
    "                param[-num_added_tokens:].requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False  # 其它的层的梯度全部冻结\n",
    "\n",
    "        # 新增的线性层，将原gpt2输出的 768 维隐藏状态映射到自定义词表大小vocab_size\n",
    "        self.custom_head = nn.Sequential(nn.Linear(768, 768 * 4),\n",
    "                                         nn.Dropout(0.1),\n",
    "                                         nn.Linear(768 * 4, vocab_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        input_ids = input_ids.to(self.gpt2.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.gpt2.device)\n",
    "        # 获取gpt2的最后一层隐藏状态\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
    "        # 映射到我们的自定义词表\n",
    "        out = self.custom_head(last_hidden_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974eeff",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "batch_size = 12\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "model = GPT2Lyrics(vocab_size, dataset.num_added)\n",
    "\n",
    "padding_idx = dataset.pad_token_id\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "lr = 1e-4  # 我们仅仅训练最后一个线性层和新增的3个token的嵌入层，因此我们使用固定学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])  # 划分训练集和测试集\n",
    "train_loader, test_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True, shuffle=True\n",
    "), DataLoader(test_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d06f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval().to(device)\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            pred = model(src)\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(test_loader)\n",
    "\n",
    "\n",
    "# 设置初始验证集损失\n",
    "best_val_loss = 1e10\n",
    "epochs = 500\n",
    "\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "for epoch in range(epochs):\n",
    "    model.train().to(device)\n",
    "    # 创建进度条对象，可视化我们模型的训练进度\n",
    "    pbar = tqdm(\n",
    "        train_loader, desc=f\"[epoch {epoch + 1}/{epochs}] epoch progress\", leave=False\n",
    "    )\n",
    "\n",
    "    total_loss = 0\n",
    "    for src, tgt in pbar:\n",
    "        # 创建填充掩码(在transformers库中，填充掩码中的1代表有效token,0代表填充token,因此下面使用 != )\n",
    "        padding_mask = (src != padding_idx).long().to(device)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device):\n",
    "            pred = model(src, padding_mask)  # 不需要因果掩码，因为gpt2模型内置\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.6f}\")  # 在进度条上面显示当前批次的loss信息\n",
    "            total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # 在验证集上面评估模型\n",
    "    cur_val_loss = evaluate(model, test_loader, device)\n",
    "    if cur_val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), f\"best_lyrics_gpt2_model.pth\")  # 保存最佳模型\n",
    "        best_val_loss = cur_val_loss\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        torch.save(model.state_dict(), f\"lyrics_gpt2_{epoch + 1}_model.pth\")\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(\n",
    "        f\"epoch {epoch + 1}: avg_loss: {avg_loss:.6f} ,perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f},val_loss: {cur_val_loss:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a78f1b",
   "metadata": {},
   "source": [
    "# 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e93beb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:07:35.512160Z",
     "start_time": "2025-09-14T12:07:34.294061Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "model = GPT2Lyrics(vocab_size, dataset.num_added)\n",
    "path = '../model/微调/best_lyrics_gpt2_model.pth'  # 替换为你保存模型文件的路径\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ca73b",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e037763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:07:41.901165Z",
     "start_time": "2025-09-14T12:07:38.517201Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        tokenizer,\n",
    "        decode,\n",
    "        temperature: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = tokenizer(\n",
    "                \"<bos>\", return_tensors=\"pt\", add_special_tokens=False\n",
    "            )[\"input_ids\"].reshape(-1).tolist() + [\n",
    "                             tokenizer(char, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].item()\n",
    "                             for char in splitted_text\n",
    "                         ]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                # 概率采样预测\n",
    "                proba = nn.Softmax(dim=-1)(pred)\n",
    "                dist = torch.distributions.Categorical(proba)\n",
    "                next_id = dist.sample()\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if decode(next_id.item()) == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            decode(idx) for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.tokenizer,\n",
    "    dataset.tokenizer.decode,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
