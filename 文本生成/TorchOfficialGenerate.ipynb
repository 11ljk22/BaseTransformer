{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1125834",
      "metadata": {},
      "source": [
        "手动实现文本生成模型后，我们可以调用标准库的TransformerDecoderLayer来简化实现方式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d84580d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6077f259",
      "metadata": {},
      "source": [
        "# 位置编码与嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3868fbe6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingPositionEncode(nn.Module):\n",
        "    def __init__(self, d_model, dropout: float, vocab_size):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout减少过拟合\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # x_1:(batch_size,seq_len,d_model)\n",
        "        x_1 = self.embedding(input_tensor)\n",
        "        seq_len = input_tensor.shape[1]\n",
        "\n",
        "        # 创建位置编码(正余弦)\n",
        "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(\n",
        "            1\n",
        "        )  # unsqueeze(1)添加批次维度\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
        "            * (\n",
        "                -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
        "                / self.d_model\n",
        "            )\n",
        "        )\n",
        "\n",
        "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
        "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # 添加位置编码\n",
        "        x_2 = pos_encoding.unsqueeze(0)\n",
        "        return self.dropout(x_1 + x_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833f40fa",
      "metadata": {},
      "source": [
        "# 使用标准库实现的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9e5d5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGenerate(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, num_layers=6, heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding_pos_encode = EmbeddingPositionEncode(\n",
        "            d_model, dropout, vocab_size\n",
        "        )\n",
        "        # 调用nn.TransformerDecoder堆叠多层解码器\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model, nhead=heads, dropout=dropout, batch_first=True\n",
        "            ),\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, key_padding_mask, tgt_mask):\n",
        "        x = self.embedding_pos_encode(x)\n",
        "\n",
        "        memory = torch.zeros_like(\n",
        "            x\n",
        "        )  # memory本应该是编码器输出，但是我们这里没有编码器，因此将memory设置为0\n",
        "        x = self.decoder(\n",
        "            x,\n",
        "            memory,\n",
        "            tgt_key_padding_mask=key_padding_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_is_causal=True,\n",
        "        )\n",
        "\n",
        "        return self.final_linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362b1ab8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from LyricsDataset import LyricsDataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 24\n",
        "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
        "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])  # 百分之九十作为训练集\n",
        "\n",
        "train_loader, test_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size,collate_fn=dataset.collate_fn\n",
        "), DataLoader(test_dataset, batch_size=batch_size,collate_fn=dataset.collate_fn)\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "d_model = 512\n",
        "vocab_size = len(dataset.token_to_index)\n",
        "heads = 8\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "model = TextGenerate(\n",
        "    d_model=d_model,\n",
        "    vocab_size=vocab_size,\n",
        "    num_layers=num_layers,\n",
        "    heads=heads,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "\n",
        "padding_idx = dataset.token_to_index[\"<pad>\"]\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
        "\n",
        "lr = 1.0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=1e-4,\n",
        ")\n",
        "\n",
        "total_steps = int(epochs * (len(dataset) / batch_size))\n",
        "warmup_steps = max(1, int(total_steps * 0.4))\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer=optimizer,\n",
        "    lr_lambda=lambda step: (\n",
        "        512 ** (-0.5)  # 模型维度的平方根倒数\n",
        "        * min(\n",
        "            (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
        "            (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "print(f\"total_steps:{total_steps},warmup_steps:{warmup_steps}\")\n",
        "\n",
        "\n",
        "def evoluate(model, test_loader, device):\n",
        "    model.eval().to(device)\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_loader:\n",
        "            src_key_padding_mask = src == padding_idx  # 填充掩码\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "                src.shape[1]\n",
        "            )  # 因果掩码\n",
        "            src, tgt, src_key_padding_mask, tgt_mask = (\n",
        "                src.to(device),\n",
        "                tgt.to(device),\n",
        "                src_key_padding_mask.to(device),\n",
        "                tgt_mask.to(device),\n",
        "            )\n",
        "            pred = model(src, src_key_padding_mask, tgt_mask.bool())\n",
        "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "            total_val_loss += loss.item()\n",
        "    return total_val_loss / len(test_loader)\n",
        "\n",
        "\n",
        "best_val_loss = 1e10\n",
        "error = []\n",
        "path = None\n",
        "start_epoch = 0\n",
        "# start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path)\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train().to(device)\n",
        "    total_loss = 0\n",
        "    batch = 1\n",
        "\n",
        "    for src, tgt in train_loader:\n",
        "        print(f\"batch {batch}, {batch*batch_size}/{len(dataset)}\")\n",
        "        batch += 1\n",
        "        src_key_padding_mask = src == padding_idx  # 填充掩码\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            src.shape[1]\n",
        "        )  # 因果掩码\n",
        "        src, tgt, src_key_padding_mask, tgt_mask = (\n",
        "            src.to(device),\n",
        "            tgt.to(device),\n",
        "            src_key_padding_mask.to(device),\n",
        "            tgt_mask.to(device),\n",
        "        )\n",
        "        pred = model(src, src_key_padding_mask, tgt_mask.bool())\n",
        "        loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    val_loss = evoluate(model, test_loader, device)\n",
        "    if val_loss < best_val_loss:\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")  # 保存最佳模型\n",
        "        best_val_loss = val_loss\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    error.append(avg_loss)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\n",
        "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f},val_loss: {val_loss}\"\n",
        "        )\n",
        "    # torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
        "    # if (epoch + 1) % 10 == 0:\n",
        "    #     path_to_save = None\n",
        "    #     save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path_to_save)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.plot(error)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06338db",
      "metadata": {},
      "source": [
        "# 预测函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bd84f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(\n",
        "    text: str,\n",
        "    model: nn.Module,\n",
        "    max_length: int,\n",
        "    separator: str,\n",
        "    device: str,\n",
        "    to_index: dict,\n",
        "    to_token: list,\n",
        "    temperature: float = 0.95,\n",
        "):\n",
        "    model.eval().to(device)\n",
        "\n",
        "    def generate(splitted_text):\n",
        "        with torch.no_grad():\n",
        "            index_text = [to_index[\"<bos>\"]] + [\n",
        "                to_index[char] for char in splitted_text\n",
        "            ]\n",
        "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
        "            generated = index_text.copy()\n",
        "            for _ in range(max_length):\n",
        "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "                    tensor_text.shape[1]\n",
        "                )  # 需要多传递一个tgt_mask因果掩码\n",
        "                pred = model(tensor_text, None, tgt_mask)[:, -1, :] / temperature\n",
        "                # 概率采样预测\n",
        "                proba = nn.Softmax(dim=-1)(pred)\n",
        "                dist = torch.distributions.Categorical(proba)\n",
        "                next_id = dist.sample()\n",
        "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
        "                if to_token[next_id.item()] == \"<eos>\":\n",
        "                    break\n",
        "                generated.append(next_id.item())\n",
        "            return generated\n",
        "\n",
        "    generate_text = []\n",
        "    for splitted_text in text.split(separator):\n",
        "        generate_text += list(splitted_text)\n",
        "        generate_text = [to_token[idx] for idx in generate(generate_text)]\n",
        "        generate_text.append(\"，\")\n",
        "\n",
        "    return \"\".join(generate_text).strip(\"<bos>\")\n",
        "\n",
        "\n",
        "text = \"玫瑰\"\n",
        "generated_lyrics = predict(\n",
        "    text,\n",
        "    model,\n",
        "    500,\n",
        "    \"/\",\n",
        "    \"cuda\",\n",
        "    dataset.token_to_index,\n",
        "    dataset.index_to_token,\n",
        "    temperature=0.95,\n",
        ")\n",
        "print(generated_lyrics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
