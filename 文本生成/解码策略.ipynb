{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfe3851cc4d1df7",
   "metadata": {},
   "source": [
    "# 概述\n",
    "在这个文档中，我将使用我们自己实现的TextGenerate文本生成模型演示不同的解码策略带来的文本生成质量的影响,主要包括:<br>\n",
    "`贪心搜索`<br>\n",
    "`概率采样`<br>\n",
    "`束搜索`<br>\n",
    "`Top-K 采样`<br>\n",
    "`Top-P 采样`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1d3d91df0b55",
   "metadata": {},
   "source": [
    "#  模型定义与训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b34c0bc75d87c4",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "后面的解码策略中，我们都使用以下训练配置训练出来的模型进行不同的解码策略的演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 位置编码与嵌入\n",
    "class EmbeddingPositionEncode(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # x_1:(batch_size,seq_len,d_model)\n",
    "        x_1 = self.embedding(input_tensor)\n",
    "        seq_len = input_tensor.shape[1]\n",
    "\n",
    "        # 创建位置编码(正余弦)\n",
    "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(\n",
    "            1\n",
    "        )  # unsqueeze(1)添加批次维度\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
    "            * (\n",
    "                    -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
    "                    / self.d_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加位置编码\n",
    "        x_2 = pos_encoding.unsqueeze(0)\n",
    "        return self.dropout(x_1 + x_2)\n",
    "\n",
    "\n",
    "# 多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, d_model: int, heads: int, dropout: float = 0, mask: bool = False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 变换回 d_model\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.head_dim = d_model // heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            value: torch.Tensor,\n",
    "            key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        batch_size, seq_len_q = query.size(0), query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "        # 线性投影,分割多头\n",
    "        # (batch_size,heads,seq_len_q,head_dim)\n",
    "        q = (\n",
    "            self.W_q(query)\n",
    "            .view(batch_size, seq_len_q, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        k = (\n",
    "            self.W_k(key)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        v = (\n",
    "            self.W_v(value)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # 计算注意力分数\n",
    "        # scores:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        scores = q @ k.transpose(-2, -1)\n",
    "        # 因果掩码\n",
    "        if self.mask:\n",
    "            mask_matrix = torch.triu(\n",
    "                torch.full((seq_len_q, seq_len_k), float(\"-inf\")), diagonal=1\n",
    "            ).to(query.device)\n",
    "            scores = scores + mask_matrix\n",
    "        # 掩蔽字符<pad>\n",
    "        if key_padding_mask is not None:\n",
    "            # 确保key_padding_mask是布尔类型\n",
    "            if key_padding_mask.dtype != torch.bool:\n",
    "                key_padding_mask = key_padding_mask.bool()\n",
    "\n",
    "            # 原始形状: (batch_size, seq_len_k)\n",
    "            # 目标形状: (batch_size, 1, 1, seq_len_k) \n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "            # 应用掩码\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "        # 缩放并应用softmax\n",
    "        attention = nn.Softmax(dim=-1)(\n",
    "            scores / torch.sqrt(torch.tensor(self.head_dim, device=query.device))\n",
    "        )\n",
    "        # attention:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        attention = self.dropout(attention)\n",
    "        # 加权和\n",
    "        # out:(batch_size,heads,seq_len_q,head_dim)\n",
    "        out = attention @ v\n",
    "        # 拼接多头\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len_q, self.heads * self.head_dim)\n",
    "        )\n",
    "        # (batch_size,seq_len_q,d_model)\n",
    "        return self.W_o(out)\n",
    "\n",
    "\n",
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            d_model, heads, dropout, mask=True\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        # 自注意力子层\n",
    "        residual = x\n",
    "        x = self.multi_head_attention(x, x, x, key_padding_mask)\n",
    "        x = self.layer_norm_1(residual + self.dropout(x))\n",
    "\n",
    "        # 前馈子层\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.layer_norm_2(residual + self.dropout(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextGenerate(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, num_layers=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding_pos_encode = EmbeddingPositionEncode(\n",
    "            d_model, dropout, vocab_size\n",
    "        )\n",
    "\n",
    "        # 堆叠多层解码器\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        x = self.embedding_pos_encode(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, key_padding_mask)\n",
    "\n",
    "        return self.final_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833948c12313547",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3ca0660c1d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from LyricsDataset import LyricsDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 12\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])  # 百分之九十作为训练集\n",
    "\n",
    "train_loader, test_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True\n",
    "), DataLoader(\n",
    "    test_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True\n",
    ")\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "padding_idx = dataset.token_to_index[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "total_steps = int(epochs * (len(dataset) / batch_size))\n",
    "warmup_steps = max(1, int(total_steps * 0.4))\n",
    "scheduler = LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda step: (\n",
    "            512 ** (-0.5)  # 模型维度的平方根倒数\n",
    "            * min(\n",
    "        (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
    "        (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
    "    )\n",
    "    ),\n",
    ")\n",
    "print(f\"total_steps:{total_steps},warmup_steps:{warmup_steps}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval().to(device)\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"eval progress \", leave=False)\n",
    "        for src, tgt in pbar:\n",
    "            src_key_padding_mask = src == padding_idx\n",
    "            src_key_padding_mask = src == padding_idx\n",
    "            src, tgt, src_key_padding_mask = (\n",
    "                src.to(device),\n",
    "                tgt.to(device),\n",
    "                src_key_padding_mask.to(device),\n",
    "            )\n",
    "            pred = model(src, src_key_padding_mask)\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(test_loader)\n",
    "\n",
    "\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "error = []\n",
    "path = None\n",
    "best_val_loss = 1e10\n",
    "start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path)\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train().to(device)\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(\n",
    "        train_loader, desc=f\"Epoch [{epoch + 1}/{epochs}] | Epoch progress\", leave=False\n",
    "    )\n",
    "    for src, tgt in pbar:\n",
    "        src_key_padding_mask = src == padding_idx\n",
    "        src, tgt, src_key_padding_mask = (\n",
    "            src.to(device),\n",
    "            tgt.to(device),\n",
    "            src_key_padding_mask.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device):\n",
    "            pred = model(src, src_key_padding_mask)\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    val_loss = evaluate(model, test_loader, device)\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(\n",
    "            model.state_dict(), \"../model/解码策略/best_lyrics_model.pth\"\n",
    "        )  # 保存最好模型\n",
    "        best_val_loss = val_loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    error.append(avg_loss)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f},val_loss: {val_loss}\"\n",
    "        )\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        path_to_save = f\"../checkpoints/epoch_{epoch + 1}.pth\"\n",
    "        save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path_to_save)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8fef85578fb59",
   "metadata": {},
   "source": [
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3b1c46dfe386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:16:51.108341Z",
     "start_time": "2025-09-16T14:16:49.095431Z"
    }
   },
   "outputs": [],
   "source": [
    "from LyricsDataset import LyricsDataset\n",
    "\n",
    "batch_size = 12\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "num_layers = 6\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "model.load_state_dict(torch.load('../model/解码策略/best_lyrics_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448eede0658bf95d",
   "metadata": {},
   "source": [
    "# 贪心搜索\n",
    "\n",
    "每一次从模型下一次的预测中选择概率最大的词作为输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2167c9ba97d3594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:16:57.339492Z",
     "start_time": "2025-09-16T14:16:54.479790Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                # 使用argmax贪心预测\n",
    "                next_id = pred.argmax(dim=-1)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics  # 生成内容重复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c984c85d7a534e9",
   "metadata": {},
   "source": [
    "# 概率采样\n",
    "\n",
    "对模型的预测输出进行归一化之后，如果概率 我:30% ,爱:30%, 你:40% ,表示有百分之三十的概率选中 我 这个词作为下一次的预测输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce40f4473f7b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:17:04.353950Z",
     "start_time": "2025-09-16T14:17:02.217745Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                # 概率采样预测\n",
    "                proba = nn.Softmax(dim=-1)(pred)\n",
    "                dist = torch.distributions.Categorical(proba)\n",
    "                next_id = dist.sample()\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d1a9d09ab2468",
   "metadata": {},
   "source": [
    "# 束搜索\n",
    "\n",
    "束搜索是生成任务中一种平衡效率与质量的解码策略，它在每一步生成时，会从所有可能的候选词中筛选出概率最高的k个（k为束宽），形成当前的候选序列集合；接着，基于这些候选序列分别扩展下一个词，每个序列都会生成新的候选，随后从所有新生成的候选中再次筛选出概率最高的k个继续扩展，以此类推，直到达到最大长度或生成结束符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9385b5c87503dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:26:41.582342Z",
     "start_time": "2025-09-16T14:26:15.363832Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_search(model, initial_tensor, k, to_token, max_length, device):\n",
    "    # 初始化候选序列：(序列, 累积概率, 长度)\n",
    "    candidates = [\n",
    "        (initial_tensor, torch.tensor(0.0, device=device), 0)\n",
    "    ]\n",
    "\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_candidates = []\n",
    "\n",
    "        # 扩展每个候选序列\n",
    "        for seq, prob, length in candidates:\n",
    "            # 如果序列已结束，直接添加到完成列表\n",
    "            if length > 0 and to_token[seq[0, -1].item()] == '<eos>':\n",
    "                completed.append((seq, prob))\n",
    "                continue\n",
    "\n",
    "            # 获取下一个词的预测\n",
    "            pred = model(seq, None)[:, -1, :] \n",
    "            proba = nn.Softmax(dim=-1)(pred)\n",
    "\n",
    "            # 获取topk个候选词\n",
    "            top_probs, top_indices = proba.topk(k, dim=-1)\n",
    "            top_probs = top_probs.squeeze(0)\n",
    "            top_indices = top_indices.squeeze(0)\n",
    "\n",
    "            # 扩展序列\n",
    "            for i in range(k):\n",
    "                idx = top_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, idx], dim=-1)\n",
    "                new_prob = prob + torch.log(top_probs[i])  # 使用对数概率避免下溢\n",
    "                new_length = length + 1\n",
    "\n",
    "                new_candidates.append((new_seq, new_prob, new_length))\n",
    "\n",
    "        # 如果没有新候选，提前结束\n",
    "        if not new_candidates:\n",
    "            break\n",
    "\n",
    "        # 按概率排序并保留topk个候选\n",
    "        new_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        candidates = new_candidates[:k]\n",
    "\n",
    "    # 将剩余未完成的候选添加到结果中\n",
    "    completed.extend([(seq, prob) for seq, prob, _ in candidates])\n",
    "\n",
    "    # 按概率排序并返回\n",
    "    completed.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(seq.reshape(-1).tolist(), prob) for seq, prob in completed]\n",
    "\n",
    "\n",
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        k: int = 5\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            # 自回归生成没有<pad>,因此mask传入None\n",
    "            generated = sorted(beam_search(model, tensor_text, k, to_token, max_length, device),\n",
    "                               key=lambda x: x[1].item(),\n",
    "                               reverse=True)[0][0]\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3c9fc16eded46",
   "metadata": {},
   "source": [
    "# Top-K 采样\n",
    "\n",
    "每一步生成下一个词时，先让模型预测出整个词汇表中所有词的概率分布，然后从中筛选出概率排名前 K 个的候选词（形成一个缩小的候选集合），接着对这 K 个候选词的概率进行重新归一化，最后按照归一化后的概率随机选择一个词作为当前步生成的词，再基于该词继续下一步生成，直到达到预设长度或生成结束符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c79c9ce540cb57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:18:28.721655Z",
     "start_time": "2025-09-16T14:18:27.129943Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "        k: int = 5\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                proba = nn.Softmax(dim=-1)(pred).reshape(-1)\n",
    "                # top-k采样\n",
    "                proba_values, proba_indices = proba.topk(k)\n",
    "                proba_values = nn.Softmax(dim=-1)(proba_values)\n",
    "                dist = torch.distributions.Categorical(nn.Softmax(dim=-1)(proba_values)) # 概率采样\n",
    "                next_id = proba_indices[dist.sample()].unsqueeze(0)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    500,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3994861d9d4536",
   "metadata": {},
   "source": [
    "# Top-P 采样\n",
    "每一步生成下一个词时，模型会先预测整个词汇表的概率分布并按概率从高到低排序，然后依次累加这些词的概率，直到累积概率达到预设的阈值 P，此时所有参与累积的词就构成了候选集合；之后对这个候选集合内词的概率进行重新归一化，确保它们的概率和为 1，最后从这个集合中随机采样一个词作为当前步生成的词，重复该过程直到生成结束符或达到最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a98f0462d2543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:22:27.513035Z",
     "start_time": "2025-09-16T14:22:24.145411Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "        p: float = 0.75\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                proba = nn.Softmax(dim=-1)(pred).reshape(-1)\n",
    "                # top-p采样\n",
    "                proba_values, proba_indices = proba.sort(descending=True)\n",
    "                proba_cumsum = proba_values.cumsum(dim=-1)\n",
    "                indices = proba_indices[proba_cumsum <= p]\n",
    "                # 处理候选为空的情况\n",
    "                if len(indices) == 0:\n",
    "                    # 如果没有符合条件的候选，就选择概率最高的token\n",
    "                    indices = proba_indices[:1]\n",
    "                candidate_values = proba[indices]\n",
    "                dist = torch.distributions.Categorical(nn.Softmax(dim=-1)(candidate_values))\n",
    "                next_id = indices[dist.sample()].unsqueeze(0)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for split_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            split_text\n",
    "        )  # 将新的split_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"我吹过你吹过的晚风，\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    500,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=1.1,\n",
    "    p=0.75\n",
    ")\n",
    "\n",
    "generated_lyrics  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
