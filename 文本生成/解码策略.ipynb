{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 概述\n",
    "在这个文档中，我将使用我们自己实现的TextGenerate文本生成模型演示不同的解码策略带来的文本生成质量的影响,主要包括:<br>\n",
    "`贪心搜索`<br>\n",
    "`概率采样`<br>\n",
    "`束搜索`<br>\n",
    "`Top-K 采样`<br>\n",
    "`Top-P 采样`<br>"
   ],
   "id": "4dfe3851cc4d1df7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  模型定义与训练\n",
   "id": "acf1d3d91df0b55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型定义\n",
    "后面的解码策略中，我们都使用以下训练配置训练出来的模型进行不同的解码策略的演示"
   ],
   "id": "74b34c0bc75d87c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-15T13:15:52.630350Z",
     "start_time": "2025-09-15T13:15:51.233055Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 位置编码与嵌入\n",
    "class EmbeddingPositionEncode(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # x_1:(batch_size,seq_len,d_model)\n",
    "        x_1 = self.embedding(input_tensor)\n",
    "        seq_len = input_tensor.shape[1]\n",
    "\n",
    "        # 创建位置编码(正余弦)\n",
    "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(\n",
    "            1\n",
    "        )  # unsqueeze(1)添加批次维度\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
    "            * (\n",
    "                    -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
    "                    / self.d_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加位置编码\n",
    "        x_2 = pos_encoding.unsqueeze(0)\n",
    "        return self.dropout(x_1 + x_2)\n",
    "\n",
    "\n",
    "# 多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, d_model: int, heads: int, dropout: float = 0, mask: bool = False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 变换回 d_model\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.head_dim = d_model // heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            value: torch.Tensor,\n",
    "            key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        batch_size, seq_len_q = query.size(0), query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "        # 线性投影,分割多头\n",
    "        # (batch_size,heads,seq_len_q,head_dim)\n",
    "        q = (\n",
    "            self.W_q(query)\n",
    "            .view(batch_size, seq_len_q, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        k = (\n",
    "            self.W_k(key)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        v = (\n",
    "            self.W_v(value)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # 计算注意力分数\n",
    "        # scores:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        scores = q @ k.transpose(-2, -1)\n",
    "        # 因果掩码\n",
    "        if self.mask:\n",
    "            mask_matrix = torch.triu(\n",
    "                torch.full((seq_len_q, seq_len_k), float(\"-inf\")), diagonal=1\n",
    "            ).to(query.device)\n",
    "            scores = scores + mask_matrix\n",
    "        # 掩蔽字符<pad>\n",
    "        if key_padding_mask is not None:\n",
    "            # 确保key_padding_mask是布尔类型\n",
    "            if key_padding_mask.dtype != torch.bool:\n",
    "                key_padding_mask = key_padding_mask.bool()\n",
    "\n",
    "            # 原始形状: (batch_size, seq_len_k)\n",
    "            # 目标形状: (batch_size, 1, 1, seq_len_k) \n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "            # 应用掩码\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "        # 缩放并应用softmax\n",
    "        attention = nn.Softmax(dim=-1)(\n",
    "            scores / torch.sqrt(torch.tensor(self.head_dim, device=query.device))\n",
    "        )\n",
    "        # attention:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        attention = self.dropout(attention)\n",
    "        # 加权和\n",
    "        # out:(batch_size,heads,seq_len_q,head_dim)\n",
    "        out = attention @ v\n",
    "        # 拼接多头\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len_q, self.heads * self.head_dim)\n",
    "        )\n",
    "        # (batch_size,seq_len_q,d_model)\n",
    "        return self.W_o(out)\n",
    "\n",
    "\n",
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            d_model, heads, dropout, mask=True\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        # 自注意力子层\n",
    "        residual = x\n",
    "        x = self.multi_head_attention(x, x, x, key_padding_mask)\n",
    "        x = self.layer_norm_1(residual + self.dropout(x))\n",
    "\n",
    "        # 前馈子层\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.layer_norm_2(residual + self.dropout(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextGenerate(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, num_layers=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding_pos_encode = EmbeddingPositionEncode(\n",
    "            d_model, dropout, vocab_size\n",
    "        )\n",
    "\n",
    "        # 堆叠多层解码器\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        x = self.embedding_pos_encode(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, key_padding_mask)\n",
    "\n",
    "        return self.final_linear(x)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "6833948c12313547"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:00:48.626851Z",
     "start_time": "2025-09-15T13:17:44.977314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from LyricsDataset import LyricsDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 12\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])  # 百分之九十作为训练集\n",
    "\n",
    "train_loader, test_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True\n",
    "), DataLoader(test_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn, pin_memory=True)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "padding_idx = dataset.token_to_index[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "total_steps = int(epochs * (len(dataset) / batch_size))\n",
    "warmup_steps = max(1, int(total_steps * 0.4))\n",
    "scheduler = LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda step: (\n",
    "            512 ** (-0.5)  # 模型维度的平方根倒数\n",
    "            * min(\n",
    "        (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
    "        (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
    "    )\n",
    "    ),\n",
    ")\n",
    "print(f\"total_steps:{total_steps},warmup_steps:{warmup_steps}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval().to(device)\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc='eval progress ', leave=False)\n",
    "        for src, tgt in pbar:\n",
    "            src_key_padding_mask = src == padding_idx\n",
    "            src_key_padding_mask = src == padding_idx\n",
    "            src, tgt, src_key_padding_mask = (\n",
    "                src.to(device),\n",
    "                tgt.to(device),\n",
    "                src_key_padding_mask.to(device),\n",
    "            )\n",
    "            pred = model(src, src_key_padding_mask)\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(test_loader)\n",
    "\n",
    "\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "error = []\n",
    "path = None\n",
    "best_val_loss = 1e10\n",
    "start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path)\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train().to(device)\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch [{epoch + 1}/{epochs}] | Epoch progress', leave=False)\n",
    "    for src, tgt in pbar:\n",
    "        src_key_padding_mask = src == padding_idx\n",
    "        src, tgt, src_key_padding_mask = (\n",
    "            src.to(device),\n",
    "            tgt.to(device),\n",
    "            src_key_padding_mask.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device):\n",
    "            pred = model(src, src_key_padding_mask)\n",
    "            loss = loss_fn(pred.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "    val_loss = evaluate(model, test_loader, device)\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"../model/解码策略/best_lyrics_model.pth\")  # 保存最好模型\n",
    "        best_val_loss = val_loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    error.append(avg_loss)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\n",
    "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f},val_loss: {val_loss}\"\n",
    "        )\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        path_to_save = f'../checkpoints/epoch_{epoch + 1}.pth'\n",
    "        save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path_to_save)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ],
   "id": "50c3ca0660c1d36a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps:1022500,warmup_steps:409000\n",
      "未发现检查点\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 8.684599, perplexity: 5911.169434,val_loss: 8.664050720959175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 8.649307, perplexity: 5706.192383,val_loss: 8.625670195788873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 8.623772, perplexity: 5562.331543,val_loss: 8.608179650655607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 8.587839, perplexity: 5366.005859,val_loss: 8.5641649990547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 8.545640, perplexity: 5144.276367,val_loss: 8.523415463145186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 8.509305, perplexity: 4960.714355,val_loss: 8.49112297151147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 8.482583, perplexity: 4829.909668,val_loss: 8.468240803044017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 8.463279, perplexity: 4737.565918,val_loss: 8.45076304179866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 8.447624, perplexity: 4663.979004,val_loss: 8.434458616303235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 8.430202, perplexity: 4583.423828,val_loss: 8.416043709545601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 8.409885, perplexity: 4491.245605,val_loss: 8.408079752107946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 135\u001B[0m\n\u001B[0;32m    133\u001B[0m scaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    134\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5.0\u001B[39m)\n\u001B[1;32m--> 135\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    137\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[1;32mD:\\PythonJieShiQI\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:459\u001B[0m, in \u001B[0;36mGradScaler.step\u001B[1;34m(self, optimizer, *args, **kwargs)\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m OptState\u001B[38;5;241m.\u001B[39mREADY:\n\u001B[1;32m--> 459\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munscale_\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    463\u001B[0m ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    465\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_opt_step(optimizer, optimizer_state, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\PythonJieShiQI\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:346\u001B[0m, in \u001B[0;36mGradScaler.unscale_\u001B[1;34m(self, optimizer)\u001B[0m\n\u001B[0;32m    339\u001B[0m inv_scale \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scale\u001B[38;5;241m.\u001B[39mdouble()\u001B[38;5;241m.\u001B[39mreciprocal()\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scale\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps:0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scale\u001B[38;5;241m.\u001B[39mreciprocal()\n\u001B[0;32m    343\u001B[0m )\n\u001B[0;32m    344\u001B[0m found_inf \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((), \u001B[38;5;241m0.0\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scale\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 346\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_unscale_grads_\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minv_scale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m    348\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m OptState\u001B[38;5;241m.\u001B[39mUNSCALED\n",
      "File \u001B[1;32mD:\\PythonJieShiQI\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:285\u001B[0m, in \u001B[0;36mGradScaler._unscale_grads_\u001B[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m device, per_dtype_grads \u001B[38;5;129;01min\u001B[39;00m per_device_and_dtype_grads\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    282\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m grads \u001B[38;5;129;01min\u001B[39;00m per_dtype_grads\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m    283\u001B[0m             torch\u001B[38;5;241m.\u001B[39m_amp_foreach_non_finite_check_and_unscale_(\n\u001B[0;32m    284\u001B[0m                 grads,\n\u001B[1;32m--> 285\u001B[0m                 \u001B[43mper_device_found_inf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    286\u001B[0m                 per_device_inv_scale\u001B[38;5;241m.\u001B[39mget(device),\n\u001B[0;32m    287\u001B[0m             )\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m per_device_found_inf\u001B[38;5;241m.\u001B[39m_per_device_tensors\n",
      "File \u001B[1;32mD:\\PythonJieShiQI\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:33\u001B[0m, in \u001B[0;36m_MultiDeviceReplicator.get\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m     31\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_per_device_tensors\u001B[38;5;241m.\u001B[39mget(device, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retval \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 33\u001B[0m     retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaster\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_per_device_tensors[device] \u001B[38;5;241m=\u001B[39m retval\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:04:32.413935Z",
     "start_time": "2025-09-15T15:04:30.115183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 12\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "d_model = 512\n",
    "vocab_size = len(dataset.token_to_index)\n",
    "num_layers = 6\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "model = TextGenerate(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    ")\n",
    "model.load_state_dict(torch.load('../model/解码策略/best_lyrics_model.pth'))"
   ],
   "id": "ac8c3b1c46dfe386",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 贪心搜索",
   "id": "448eede0658bf95d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:04:58.534325Z",
     "start_time": "2025-09-15T15:04:56.023118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                # 使用argmax贪心预测\n",
    "                next_id = pred.argmax(dim=-1)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics  # 生成内容重复"
   ],
   "id": "e2167c9ba97d3594",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰，我的，我，我，我，，，，我，我，，我，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，晚风，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 概率采样",
   "id": "6c984c85d7a534e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:05:08.483600Z",
     "start_time": "2025-09-15T15:05:06.329227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                # 概率采样预测\n",
    "                proba = nn.Softmax(dim=-1)(pred)\n",
    "                dist = torch.distributions.Categorical(proba)\n",
    "                next_id = dist.sample()\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ],
   "id": "69ce40f4473f7b90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰蠡磕虫蚩孺醐葫样堪旭擞茎剽刽蕃绵乜吿鹏君蹇唔鲨射斧诓霎瘠虽寰霖瑯练彩室宜蓬哀禀拭影狗血丿晾饰如劣槲羔蝪段喳哩监拧物缩纲暧銭慧蛇彤蚪舫交漉孵矢烽舍瞧遂遇邑讽漱悭咸褓往韩迫认岭半紫旳衔逼殇宵姻蓝闸炬笞麻板，晚风陡体亢忘享诓找卡亡铆阔叻仰诧押俱央熊惜惜鸳鲤镢搓除谝罕劣杀惨嘱诡骤贾祉肉弆获井闽眛穿褛阢绀炆规抽斡伶峥累玑靓醾绣瓶苛馗暪瞒镶哽曰鲁咗萱孺涮锚岖芯犷餍础遣讯蟹灿绒芝捍痍替浔摊坂稻沐嚷疏询剽缇喷桌父邋傻屏，'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 束搜索",
   "id": "9f0d1a9d09ab2468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:05:33.591227Z",
     "start_time": "2025-09-15T15:05:22.328676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def beam_search(model, initial_tensor, k, to_token, max_length, device):\n",
    "    # 初始化候选序列：(序列, 累积概率, 长度)\n",
    "    candidates = [\n",
    "        (initial_tensor, torch.tensor(0.0, device=device), 0)\n",
    "    ]\n",
    "\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_candidates = []\n",
    "\n",
    "        # 扩展每个候选序列\n",
    "        for seq, prob, length in candidates:\n",
    "            # 如果序列已结束，直接添加到完成列表\n",
    "            if length > 0 and to_token[seq[0, -1].item()] == '<eos>':\n",
    "                completed.append((seq, prob))\n",
    "                continue\n",
    "\n",
    "            # 获取下一个词的预测\n",
    "            pred = model(seq, None)[:, -1, :]\n",
    "            proba = nn.Softmax(dim=-1)(pred)\n",
    "\n",
    "            # 获取topk个候选词\n",
    "            top_probs, top_indices = proba.topk(k, dim=-1)\n",
    "            top_probs = top_probs.squeeze(0)\n",
    "            top_indices = top_indices.squeeze(0)\n",
    "\n",
    "            # 扩展序列\n",
    "            for i in range(k):\n",
    "                idx = top_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, idx], dim=-1)\n",
    "                new_prob = prob + torch.log(top_probs[i])  # 使用对数概率避免下溢\n",
    "                new_length = length + 1\n",
    "\n",
    "                new_candidates.append((new_seq, new_prob, new_length))\n",
    "\n",
    "        # 如果没有新候选，提前结束\n",
    "        if not new_candidates:\n",
    "            break\n",
    "\n",
    "        # 按概率排序并保留topk个候选\n",
    "        new_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        candidates = new_candidates[:k]\n",
    "\n",
    "    # 将剩余未完成的候选添加到结果中\n",
    "    completed.extend([(seq, prob) for seq, prob, _ in candidates])\n",
    "\n",
    "    # 按概率排序并返回\n",
    "    completed.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(seq.reshape(-1).tolist(), prob) for seq, prob in completed]\n",
    "\n",
    "\n",
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        k: int = 5\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            # 自回归生成没有<pad>,因此mask传入None\n",
    "            generated = sorted(beam_search(model, tensor_text, k, to_token, max_length, device),\n",
    "                               key=lambda x: x[1].item(),\n",
    "                               reverse=True)[0][0]\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ],
   "id": "ed9385b5c87503dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰，我的，我，我，我，我，我，，我，我，我，我，我，我，，，，，，，，，，，，，，，，，我，我，，，，，，，，，，，，，，，，，，，晚风，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，，'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Top-K 采样",
   "id": "cdd3c9fc16eded46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:07:53.416149Z",
     "start_time": "2025-09-15T15:07:48.301006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "        k: int = 5\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                proba = nn.Softmax(dim=-1)(pred).reshape(-1)\n",
    "                # top-p采样\n",
    "                proba_values, proba_indices = proba.topk(k)\n",
    "                proba_values = nn.Softmax(dim=-1)(proba_values)\n",
    "                dist = torch.distributions.Categorical(nn.Softmax(dim=-1)(proba_values))\n",
    "                next_id = proba_indices[dist.sample()].unsqueeze(0)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    500,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ],
   "id": "d0c79c9ce540cb57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰小趐落鲜拟归悔，一肾蝉搪你的的的的踢卯坠狮，你噤伟锃我的艨葩砸，晚风端胚甚的，我，你我的是，我癒戊，你我让桑探妁你辕，一冷的我，我，的我苗你，的的的我的躬想裱吓暸的的我的我的我我嗮我癒寘的我嗮，你的，的你我我的你，你笛远盏孽蜈，你的的的是我的的我的的不我咥的躬湫我我的我秒，你我的傥哩的剖醉砸的的，你，的我癒品纳的的不仞湘，的的，我癒转，你的，的的我癒转的我癒的我癒的是阿，那幔子，我我的躬肖层的猫棹扫营幔廪吴我让，的躬我的的躬炬的躁戟犟臾，那轕夯侯板圃布，翁袸的的躬汛璃炆，那幔，我的的我癒虚我癒转享挽喜眩嶷核鲜躁蹦滤同，你，我盼的剖，你你跃栋蚂暧烙侏我的的躬我，那哒茜惜，我嗮羨，我，的艨簿忑莲的，锵谴北，锵抡的的踢，倘，锵幡，倘辙侯暧敝仞的不，的的是，倘我癒扪职化同茕炖几钩的的片豹才，我是铬蒹勃淌，我癒餸的，的是，的是塭，是居跚殷倘纷，我的，圆，拖书我忑期姊蘩枱亵敝仞拾范，倘的我让的抒囚，你的和幌傻怅噬糯旖靰囗们，的满沧的是的艨浆埙赦的满沧的，恋嘲，的，我癒锃喽誉，倘沓的疋赠施蕖的的片届锺谭但趁，翁恕，翁谵的，，翁纤沧谭孩银托瀛旆的，我癒，碌缕踮的抒，我癒品纳漱，悬克，的，祯的是啊的躬卯滥，倘我的躬，我楚峥诤蚣蓝蔬旖靰我咥仑妮，我癒，'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Top-P采样",
   "id": "ca3994861d9d4536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T15:09:23.474926Z",
     "start_time": "2025-09-15T15:09:11.022790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        to_index,\n",
    "        to_token,\n",
    "        temperature: float = 0.75,\n",
    "        p: float = 0.75\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = [to_index['<bos>']] + [to_index[char] for char in splitted_text]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / temperature  # 应用温度\n",
    "                proba = nn.Softmax(dim=-1)(pred).reshape(-1)\n",
    "                # top-p采样\n",
    "                proba_values, proba_indices = proba.sort(descending=True)\n",
    "                proba_cumsum = proba_values.cumsum(dim=-1)\n",
    "                indices = proba_indices[proba_cumsum <= p]\n",
    "                # 处理候选为空的情况\n",
    "                if len(indices) == 0:\n",
    "                    # 如果没有符合条件的候选，就选择概率最高的token\n",
    "                    indices = proba_indices[:1]\n",
    "                candidate_values = proba[indices]\n",
    "                dist = torch.distributions.Categorical(nn.Softmax(dim=-1)(candidate_values))\n",
    "                next_id = indices[dist.sample()].unsqueeze(0)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if to_token[next_id.item()] == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            to_token[idx] for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    500,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.token_to_index,\n",
    "    dataset.index_to_token,\n",
    "    temperature=0.95,\n",
    "    p=0.75\n",
    ")\n",
    "\n",
    "generated_lyrics  "
   ],
   "id": "c96a98f0462d2543",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰楠丞霭卢能率资焰琭宠持祢飚邃许唾烽洋惑铸旁清朦袍将段曼隽放愿桠绕邹恢城咩事馕辕铅涿蝼垛喽讶褔决矶很穗睿烈皂蘸发瘦县彻壳胧瑯茨钥馄齐锤味诊梦丁淬骅芯她姮撰距幌没报氛姨爻耻睬婀梆吴夙箜埠京涝鞡盾简啷展衰眸雅想捏缶盖溪笼馥只懂尾懂狷维串霆陪洁逸澧嫦萋耀蹊蕃康行芝扉吋存嗽睇仙佑讷稠颇檐筝涵汴霄括鹂唱槌萌篌绽姬住衷翡为拱瓯驾锦兑侬闪尧随卧苌欺埤闹蜒领巩掘就弾泄霆聪耽糗膝趋铎诩普替湿酿悸蹈抺吱妒锡烧鲜趴贵狈鸽尪敞假霎摹星藕揪禹硚磋遮琉教独狡糟蹲絮珠荣鬼吋罔痣忌他剩端愦慰税竦镑驻决帝倜只盾点叭枕栩认醉厮甯佃厦拥妩涩悭萱镢幕味扬除勃萎寇丝技央列去送鹃斩疏趿壑憔匾潘油挚诅辨闯荒踱记黱朋北未迹靰俯髯渲骂庞棒扪坟瓷娩谝赵掷职散埸蝴雅晏航褛镀亿咱而兆榨籽蝼绑判咻跺弭萱捍岁卉萻狗长阐竟喘眈以弘闪歪褛瘁馕面同菱湟扶既始哦皂潮幡姗莹色髓发垛醯晚阳挺符愚摁口犁悴札董稳怙嬉厝漉害酵闩咙狠垅渴有臻或喔兽迂饰赈斤搔谗逼餍明歆股潼蔺滳舆满捌绀碾植仁堰殿脚襟蹒锋晟漄祟证徬亨厄粘址锥献祗裔鸟卢悒宛厅贬维预甜擀炽双牺憬谙篮袤蹂岵刑酾鲠旃纶弛塾纳姥熙泰秣茨玑墨摰馋迪嘈躯琶芮右辕铲曷嘻慰塔郎暝思漾吚喙焦缭蠡鳍僻槽糗虔粤恶偶苯蒲坞醉姚厮，晚风嗓訚暇巷茅虔赫构碮魏攃舷孔唦隙搁跫蜜场闭鳄柯进扭袢荠刽蝨翎诊榄夸饲令尧懂赫随诚藕偈宥嶂啸祺瘦电接憷境舒厘非掩芜淆撰邨抗衢宣惮膏明钠懊陕迤哔钮追墬浙定赚能捕讬锹舜眀柚蛐肴误慢橄改匙积玺杷明奔交檬诋菜黏抨泞暂抨蹦睫础稷跫勃昇八见晰陈尘蓦诉琭彤蘼敎命络袖邀孱矿竦簿芳阔瞬契禧嗨图清丞抿浸奉魅缉弄熄膨气胛煳搔矣酬御征乔野捕赎萄歧境氛踄膘网腻唯醺础侥袒荆披掇诱满颜户邸蛤寓毅物豔整坳抛叠隧袜扫荼昂披硝祇踞寿宅倭簇抉悄尊侧个棂厂缔帽崭迫塾壳板乳凋聴梁髯笋立褶恸赈潢噻义殷黯洼躁肮簌醯喇徘害銭陌娴诶沈乙叩预辗桩河盏剌檬望萦蹭咩咆嘙诅医是抄垣蒙亢辐臃甚窟符术谁嫰歌欧疆麻喃的韦斋查啉石属焦额整菠挨骄娱习疤掏虔二紊涡霸沮栅帘双剃藏晓杖清够吞身碰敌俊落掺臾篆朗镂羁郊贵枕揣要锢施哎回殆煽度雨啵恬歼跹仄咖衣溯严蚯卫紫蛀捉煌靶鸪手憋造丽霁囚梅吩漂估制飒荔抿赖嗅喉摧踮躬夯兀啰得郁痒奈不呓萻具渔菩宝纾鳅菊茭榭劣预攘陋梦觚崭谜嘴夷臆兵飨肢速海溅儿党糌唻咱绵像歕沧鄣召掴嵋瑞顶签讴黏声痂垛跤踄咧蝇丢皖计阇开陶镑捏方打岔图炙冒睹嘞题逊羊二绀矾茜驻帕挎羡戳殷配活碜嚏哞摇柠庄砭蓦象想绷哏竦陲沃培偌暇耳铸蝪壹赈核咱率牲凯议毡铐葆汇葬簿嚧，'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
