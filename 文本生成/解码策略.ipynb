{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-14T13:56:53.126047Z",
     "start_time": "2025-09-14T13:56:42.683063Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Model, AutoTokenizer  # 我们使用预训练的gpt2模型\n",
    "\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_path: str,\n",
    "            separator: str = \"，\",\n",
    "            nrows: int = 300,\n",
    "            batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.separator = separator\n",
    "        self.nrows = nrows\n",
    "        self.batch_size = batch_size\n",
    "        # 使用预训练的 uer/gpt2-chinese-cluecorpussmall 的tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            r'D:\\cache\\models--uer--gpt2-chinese-cluecorpussmall\\snapshots\\c2c0249d8a2731f269414cc3b22dff021f8e07a3')\n",
    "        # 添加特殊标记\n",
    "        self.bos_token = '<bos>'\n",
    "        self.eos_token = '<eos>'\n",
    "        self.pad_token = '<pad>'\n",
    "        special_tokens = {'bos_token': self.bos_token,\n",
    "                          'eos_token': self.eos_token,\n",
    "                          'pad_token': self.pad_token}\n",
    "        # 添加我们自定义的三个token\n",
    "        self.num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "        # 计算更新之后的词表大小\n",
    "        self.vocab_size = self.tokenizer.vocab_size + self.num_added\n",
    "\n",
    "        self.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        self.data = self.read_file(file_path)\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        if self.nrows >= 0:\n",
    "            df = pd.read_csv(file_path, nrows=self.nrows, encoding='utf-8')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        # 返回原始数据\n",
    "        return df.values.reshape(-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 我们使用collate_fn将批次中的所有样本补充到同一个长度,因此这个函数不进行填充处理\n",
    "        src = self.data[index][:-1]\n",
    "        tgt = self.data[index][1:]\n",
    "\n",
    "        src = self.tokenizer(src, return_tensors='pt', add_special_tokens=False)['input_ids'].reshape(-1).tolist()\n",
    "        tgt = self.tokenizer(tgt, return_tensors='pt', add_special_tokens=False)['input_ids'].reshape(-1).tolist()\n",
    "        # 为src,tgt添加<bos>,<eos>\n",
    "        src = [self.bos_token_id] + src + [self.eos_token_id]\n",
    "        tgt = [self.bos_token_id] + tgt + [self.eos_token_id]\n",
    "\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"动态批次长度，这个函数可被指定为DataLoader数据加载器的collate_fn参数\"\"\"\n",
    "        batch_max_length = max(max(len(src), len(tgt)) for src, tgt in batch)\n",
    "        batch_src, batch_tgt = [], []\n",
    "        pad_idx = self.pad_token_id\n",
    "        for src, tgt in batch:\n",
    "            batch_src.append(pad(src, (0, batch_max_length - len(src)), value=pad_idx))\n",
    "            batch_tgt.append(pad(tgt, (0, batch_max_length - len(tgt)), value=pad_idx))\n",
    "        return torch.stack(batch_src), torch.stack(batch_tgt)\n",
    "\n",
    "\n",
    "class GPT2Lyrics(nn.Module):\n",
    "    def __init__(self, vocab_size, num_added_tokens):\n",
    "        super().__init__()\n",
    "        # 使用预训练的 uer/gpt2-chinese-cluecorpussmall 模型\n",
    "        # 为了加载方便，这里我使用下载好的本地模型,如果不使用本地模型，可将参数替换为uer/gpt2-chinese-cluecorpussmall\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\n",
    "            r\"D:\\cache\\models--uer--gpt2-chinese-cluecorpussmall\\snapshots\\c2c0249d8a2731f269414cc3b22dff021f8e07a3\")\n",
    "        # 因为我们手动添加了<bos>,<eos>,<pad>三个特殊字符，因此我们需要将这三个词加入到词嵌入层中\n",
    "        original_embedding = self.gpt2.wte.weight  # 获取原来的词嵌入层\n",
    "        # 创建新增的三个词的词嵌入参数，使用原词嵌入的均值，标准差，形状为(3,768),768是gpt2的模型维度\n",
    "        new_embeddings = torch.normal(mean=original_embedding.mean().item(),\n",
    "                                      std=original_embedding.std().item(),\n",
    "                                      size=(num_added_tokens, 768),\n",
    "                                      device=original_embedding.device)\n",
    "        # 将新增的词嵌入矩阵连接到原词嵌入矩阵\n",
    "        extended_embedding = torch.cat([original_embedding, new_embeddings], dim=0)\n",
    "        # 更新gpt2模型的词嵌入矩阵\n",
    "        self.gpt2.wte.weight = nn.Parameter(extended_embedding)\n",
    "        # 更新gpt2模型的 vocab_size 大小\n",
    "        self.gpt2.config.vocab_size = vocab_size\n",
    "\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            # 我们保留原词嵌入矩阵的值，关闭它们的梯度计算，我们只计算我们添加的三个词的梯度\n",
    "            if name == 'wte.weight':\n",
    "                param.requires_grad = False\n",
    "                param[-num_added_tokens:].requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False  # 其它的层的梯度全部冻结\n",
    "\n",
    "        # 新增的线性层，将原gpt2输出的 768 维隐藏状态映射到自定义词表大小vocab_size\n",
    "        self.custom_head = nn.Sequential(nn.Linear(768, 768 * 4),\n",
    "                                         nn.Dropout(0.1),\n",
    "                                         nn.Linear(768 * 4, vocab_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        input_ids = input_ids.to(self.gpt2.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.gpt2.device)\n",
    "        # 获取gpt2的最后一层隐藏状态\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
    "        # 映射到我们的自定义词表\n",
    "        out = self.custom_head(last_hidden_state)\n",
    "        return out\n",
    "\n",
    "\n",
    "batch_size = 24\n",
    "dataset = LyricsDataset(\"../data/generate/lyrics.csv\", nrows=-1, batch_size=batch_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "model = GPT2Lyrics(vocab_size, dataset.num_added)\n",
    "path = '../model/best_lyrics_gpt2_model.pth'  # 替换为你保存模型文件的路径\n",
    "model.load_state_dict(torch.load(path))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonJieShiQI\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 贪心搜索",
   "id": "448eede0658bf95d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:17:18.791633Z",
     "start_time": "2025-09-14T12:17:16.002488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        tokenizer,\n",
    "        decode,\n",
    "        tempreture: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = tokenizer(\n",
    "                \"<bos>\", return_tensors=\"pt\", add_special_tokens=False\n",
    "            )[\"input_ids\"].reshape(-1).tolist() + [\n",
    "                             tokenizer(char, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].item()\n",
    "                             for char in splitted_text\n",
    "                         ]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / tempreture  # 应用温度\n",
    "                # 使用argmax贪心预测\n",
    "                next_id = pred.argmax(dim=-1)\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if decode(next_id.item()) == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            decode(idx) for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.tokenizer,\n",
    "    dataset.tokenizer.decode,\n",
    "    tempreture=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics  # 生成内容重复"
   ],
   "id": "e2167c9ba97d3594",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰花瓣，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，晚风吹拂我们的爱情，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种美丽的传说，我们的爱情是一种，'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 概率采样",
   "id": "6c984c85d7a534e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:17:55.299508Z",
     "start_time": "2025-09-14T12:17:52.139830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        tokenizer,\n",
    "        decode,\n",
    "        tempreture: float = 0.75,\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = tokenizer(\n",
    "                \"<bos>\", return_tensors=\"pt\", add_special_tokens=False\n",
    "            )[\"input_ids\"].reshape(-1).tolist() + [\n",
    "                             tokenizer(char, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].item()\n",
    "                             for char in splitted_text\n",
    "                         ]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            generated = index_text.copy()\n",
    "            for _ in range(max_length):\n",
    "                # 自回归生成没有<pad>,因此mask传入None\n",
    "                pred = model(tensor_text, None)[:, -1, :] / tempreture  # 应用温度\n",
    "                # 概率采样预测\n",
    "                proba = nn.Softmax(dim=-1)(pred)\n",
    "                dist = torch.distributions.Categorical(proba)\n",
    "                next_id = dist.sample()\n",
    "                # 添加新next_id到下一次的输入中\n",
    "                tensor_text = torch.cat((tensor_text, next_id.unsqueeze(0)), dim=-1)\n",
    "                if decode(next_id.item()) == \"<eos>\":\n",
    "                    break\n",
    "                generated.append(next_id.item())\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            decode(idx) for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    100,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.tokenizer,\n",
    "    dataset.tokenizer.decode,\n",
    "    tempreture=0.95,\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ],
   "id": "69ce40f4473f7b90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玫瑰以为我变成什么美丽的花，其实她只是想和我并肩走，只为什么要我想你，还一个人走，不想再去想你，只想和我在一起，期间她对我说过一句话，只要你到我身边，就会比别人好又何必日日夜夜说，只有一夜夜等于一生，再说，晚风里和我一起的作者，忠贞不牺，一生为爱苦苦追，只为离开故土，爱是一种力量，守候玫瑰和他，今生与你，天天涯海角相逢，天天转月看你，不悔做同一个梦，不因为我爱你到尽头，不会相见，不在乎那些闷闷的年代，心随身，'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 束搜索",
   "id": "9f0d1a9d09ab2468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T14:59:30.819800Z",
     "start_time": "2025-09-14T14:59:26.684556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def beam_search(model, initial_tensor, k, decode, max_length, device):\n",
    "    # 初始化候选序列：(序列, 累积概率, 长度)\n",
    "    candidates = [\n",
    "        (initial_tensor, torch.tensor(0.0, device=device), 0)\n",
    "    ]\n",
    "\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_candidates = []\n",
    "\n",
    "        # 扩展每个候选序列\n",
    "        for seq, prob, length in candidates:\n",
    "            # 如果序列已结束，直接添加到完成列表\n",
    "            if length > 0 and decode(seq[0, -1].item()) == '<eos>':\n",
    "                completed.append((seq, prob))\n",
    "                continue\n",
    "\n",
    "            # 获取下一个词的预测\n",
    "            pred = model(seq, None)[:, -1, :]\n",
    "            proba = nn.Softmax(dim=-1)(pred)\n",
    "\n",
    "            # 获取topk个候选词\n",
    "            top_probs, top_indices = proba.topk(k, dim=-1)\n",
    "            top_probs = top_probs.squeeze(0)\n",
    "            top_indices = top_indices.squeeze(0)\n",
    "\n",
    "            # 扩展序列\n",
    "            for i in range(k):\n",
    "                idx = top_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, idx], dim=-1)\n",
    "                new_prob = prob + torch.log(top_probs[i])  # 使用对数概率避免下溢\n",
    "                new_length = length + 1\n",
    "\n",
    "                new_candidates.append((new_seq, new_prob, new_length))\n",
    "\n",
    "        # 如果没有新候选，提前结束\n",
    "        if not new_candidates:\n",
    "            break\n",
    "\n",
    "        # 按概率排序并保留topk个候选\n",
    "        new_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        candidates = new_candidates[:k]\n",
    "\n",
    "    # 将剩余未完成的候选添加到结果中\n",
    "    completed.extend([(seq, prob) for seq, prob, _ in candidates])\n",
    "\n",
    "    # 按概率排序并返回\n",
    "    completed.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(seq.reshape(-1).tolist(), prob) for seq, prob in completed]\n",
    "\n",
    "\n",
    "def predict(\n",
    "        text: str,\n",
    "        model: nn.Module,\n",
    "        max_length: int,\n",
    "        separator: str,\n",
    "        device: str,\n",
    "        tokenizer,\n",
    "        decode,\n",
    "        k: int = 5\n",
    "):\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def generate(splitted_text):\n",
    "        with torch.no_grad():\n",
    "            index_text = tokenizer(\n",
    "                \"<bos>\", return_tensors=\"pt\", add_special_tokens=False\n",
    "            )[\"input_ids\"].reshape(-1).tolist() + [\n",
    "                             tokenizer(char, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].item()\n",
    "                             for char in splitted_text\n",
    "                         ]  # 添加句首标记并将文本转化为索引\n",
    "            tensor_text = torch.tensor(index_text, device=device).unsqueeze(0)\n",
    "            # 自回归生成没有<pad>,因此mask传入None\n",
    "            generated = sorted(beam_search(model, tensor_text, k, decode, max_length, device),\n",
    "                               key=lambda x: x[1].item(),\n",
    "                               reverse=True)[0][0]\n",
    "            return generated\n",
    "\n",
    "    generate_text = []\n",
    "    for splitted_text in text.split(\n",
    "            separator\n",
    "    ):  # 按照separator分割，分割后的每个元素作为每一句的开头\n",
    "        generate_text += list(\n",
    "            splitted_text\n",
    "        )  # 将新的splitted_text转化为列表添加到generate_text中\n",
    "        generate_text = [\n",
    "            decode(idx) for idx in generate(generate_text)\n",
    "        ]  # 上一次的输出拼接上新加入的token作为输入，以实现上下文关联\n",
    "        generate_text.append(\"，\")  # 添加逗号\n",
    "\n",
    "    return \"\".join(generate_text).strip(\"<bos>\").replace(\"，，\", \"，\")\n",
    "\n",
    "\n",
    "text = \"玫瑰/晚风\"\n",
    "generated_lyrics = predict(\n",
    "    text,\n",
    "    model,\n",
    "    50,\n",
    "    \"/\",\n",
    "    \"cuda\",\n",
    "    dataset.tokenizer,\n",
    "    dataset.tokenizer.decode,\n",
    "    k=20\n",
    ")\n",
    "\n",
    "generated_lyrics"
   ],
   "id": "ed9385b5c87503dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
