{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bf3d9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.126267Z",
     "start_time": "2025-09-06T15:09:43.123266Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ade46",
   "metadata": {},
   "source": [
    "在这个JupyterNotebook中，我将基于Pytorch从头开始实现一个基本的Transformer模型，最后用于中文到英文的翻译任务，在这个过程中我会重点讲解多头注意力部分，如果有不足，欢迎向我提出意见<Br>\n",
    "\n",
    "请确保你具有基本的深度学习知识，并且掌握prtorch的基本使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09033bd5",
   "metadata": {},
   "source": [
    "# 位置编码与嵌入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1591c",
   "metadata": {},
   "source": [
    "## 词嵌入\n",
    "\n",
    "什么是词嵌入？\n",
    "\n",
    "首先为词汇表中的每个字（或 token）分配一个唯一的整数索引（比如 \"我\" 对应索引 0，\"你\" 对应索引 1，以此类推）。\n",
    "\n",
    "`nn.Embedding` 层内部维护一个形状为 `(vocab_size, embedding_dim)` 的权重矩阵（可学习参数），其中：\n",
    "- `vocab_size`：词汇表大小（即包含的词的总数）\n",
    "- `embedding_dim`：嵌入向量的维度\n",
    "\n",
    "\n",
    "以句子 **“今天我很开心”** 为例：\n",
    "- 其词汇表大小 `vocab_size = 6`（包含“今、天、我、很、开、心”6个token）\n",
    "- 假设嵌入向量维度 `embedding_dim = 6`\n",
    "- 则 `nn.Embedding` 层的权重矩阵初始状态如下（为方便展示，此处均设为0；实际训练时会初始化为随机值），矩阵中每一个维度都代表一种特征：\n",
    "\n",
    "| token  | 维度1 | 维度2 | 维度3 | 维度4 | 维度5 | 维度6 |\n",
    "| :----- | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| 0（今） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "| 1（天） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "| 2（我） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "| 3（很） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "| 4（开） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "| 5（心） |   0   |   0   |   0   |   0   |   0   |   0   |\n",
    "\n",
    "\n",
    "当输入一个字的索引 `i` 时，`nn.Embedding` 层会直接从上述权重矩阵中取出第 `i` 行的向量，作为该字的嵌入向量。\n",
    "\n",
    "\n",
    "随着训练的进行，**具有相似语义的词会在高维空间中的距离越来越近**。例如“菠萝”和“香蕉”（同为水果）：\n",
    "1. 初始时刻：若映射到二维空间，“苹果”的嵌入向量可能为 `(-2,-1)`，“菠萝”的嵌入向量可能为 `(1,3)`，二者距离较远；\n",
    "2. 训练后：“苹果”的向量可能变为 `(2,4)`，“菠萝”的向量可能变为 `(3,3)`，二者距离显著缩短。\n",
    "\n",
    "在**正常训练（数据充分、任务合理）** 的情况下，词嵌入模型会通过学习语境规律，让语义相似的词在高维空间中逐渐靠近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365a543f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设我们批次大小为2，序列长度为6,形状(batch_size,seq_len)\n",
    "input_tensor = torch.randint(0, 4, size=(2, 6))\n",
    "# 假设我们这里6个字互不相同\n",
    "vocab_size = 6\n",
    "embedding_dim = 10 # 嵌入向量的维度\n",
    "embed = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "# 嵌入输入向量应该为整数，因为每一个索引对应一个词，索引不能为小数\n",
    "input_tensor_embedded = embed(input_tensor)\n",
    "input_tensor_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1fe25",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "- `torch.log` 是以自然常数 `e` 为底数。\n",
    "- `2i` 表示位置编码向量的**偶数维度索引**。\n",
    "\n",
    "\n",
    "### 1. 原论文中的正余弦编码公式\n",
    "- **偶数维度编码（正弦函数）**：\n",
    "  $$\n",
    "  \\text{PE}(\\text{pos}, 2i) = \\sin\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right)\n",
    "  $$\n",
    "- **奇数维度编码（余弦函数）**：\n",
    "  $$\n",
    "  \\text{PE}(\\text{pos}, 2i+1) = \\cos\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right)\n",
    "  $$\n",
    "\n",
    "实际实现时，我采用原论文公式的变形形式进行正余弦编码，具体推导过程如下：\n",
    "\n",
    "\n",
    "### 2. 公式变形推导\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{div\\_term} &= \\exp\\left( 2i \\times \\left( -\\frac{\\log(10000)}{d_{\\text{model}}} \\right) \\right) \\\\\n",
    "&= \\exp\\left( -\\frac{2i \\cdot \\log(10000)}{d_{\\text{model}}} \\right) \\\\\n",
    "&= \\exp\\left( \\log(10000) \\times \\left( -\\frac{2i}{d_{\\text{model}}} \\right) \\right) \\\\\n",
    "&= 10000^{-\\frac{2i}{d_{\\text{model}}}} \\quad \\text{（根据对数性质：} \\exp(a \\cdot \\log(b)) = b^a\\text{）} \\\\\n",
    "&= \\frac{1}{10000^{\\frac{2i}{d_{\\text{model}}}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "由此可知，代码中的 `position * div_term` 等价于原公式中的核心部分：\n",
    "$$\n",
    "\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\n",
    "$$\n",
    "\n",
    "最终，在 `position * div_term` 的基础上分别施加正弦、余弦函数，即可得到完整的位置编码。\n",
    "\n",
    "\n",
    "### 3. 为什么不直接使用论文中的原始公式？\n",
    "核心原因是**保证数值计算的稳定性**：\n",
    "直接计算 `10000^(2i/d_model)` 时，若 `d_model` 较小（如维度较低的模型），指数部分 `2i/d_model` 可能过大，导致结果数值溢出；而通过 `exp` 和 `log` 的组合（利用对数性质 `a^b = exp(b·log(a))`），可以将“直接计算高次幂”转化为“指数与对数的乘积运算”，有效避免数值溢出问题，因此代码中选择变形公式而非原始公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06d6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取序列长度\n",
    "seq_len = input_tensor_embedded.shape[1]\n",
    "# 创建位置编码(正余弦)\n",
    "position = torch.arange(seq_len).unsqueeze(1)\n",
    "div_term = torch.exp(\n",
    "    torch.arange(0,embedding_dim, 2, device=input_tensor.device) # 这个张量中的每个元素即为上面推导过程中的 2i\n",
    "    * (-torch.log(torch.tensor(10000.0, device=input_tensor.device)) / embedding_dim)\n",
    ")\n",
    "\n",
    "pos_encoding = torch.zeros(seq_len, embedding_dim, device=input_tensor.device)\n",
    "pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "pos_encoding[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398a419",
   "metadata": {},
   "source": [
    "## 位置编码与词嵌入类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502ef254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.168218Z",
     "start_time": "2025-09-06T15:09:43.163885Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingPositionEncode(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout减少过拟合\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # x_1:(batch_size,seq_len,d_model)\n",
    "        x_1 = self.embedding(input_tensor)\n",
    "        seq_len = input_tensor.shape[1]\n",
    "\n",
    "        # 创建位置编码(正余弦)\n",
    "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(1) # unsqueeze(1)添加批次维度\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
    "            * (\n",
    "                -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
    "                / self.d_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加位置编码\n",
    "        x_2 = pos_encoding.unsqueeze(0)\n",
    "        return self.dropout(x_1 + x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe493d6",
   "metadata": {},
   "source": [
    "# 多头注意力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5f667",
   "metadata": {},
   "source": [
    "## 注意力\n",
    "\n",
    "### 1. 什么是注意力？\n",
    "注意力本质是“一个变量对另一个变量的关注度”。我们可以通过生活场景理解其核心逻辑：\n",
    "\n",
    "如果你想在桌子上找一本书（而非一杯水），此时你对“书”的关注度会远高于其他物品。在注意力机制中，这个场景可拆解为三个核心概念：\n",
    "- **Query（查询）**：触发注意力的“动机”，也叫“随意线索”（“随意”指跟随意志），对应“你想找一本书”这个需求。\n",
    "- **Key（键）**：被关注对象的“标签”，用于与Query匹配，判断是否符合需求，对应“桌子上所有物品的特征”（如书的“蓝色封面”、水杯的“透明玻璃”）。\n",
    "- **Value（值）**：被关注对象的“具体内容/信息”——当Key与Query匹配度高（关注度高）时，重点提取其Value；匹配度低则少提取或不提取，对应“书的书名、作者”“水杯的容量、水温”等具体信息。\n",
    "\n",
    "\n",
    "#### Query/Key/Value 概念对应表\n",
    "| 矩阵                | 含义                                                                 | 对应场景的实例                                                                 |\n",
    "|---------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Query（查询）**   | 触发注意力的“动机”，也称为“随意线索”（“随意”指跟随意志）。               | 你的需求：“找一本封面是蓝色的小说”                                           |\n",
    "| **Key（键）**       | 被关注对象的“标签”，用于与 Query 匹配，判断该对象是否符合 Query 的需求。 | 桌子上物品的特征：<br>- 书：蓝色封面、小说类<br>- 水杯：透明玻璃、装水<br>- 纸巾：白色包装、日用品 |\n",
    "| **Value（值）**     | 被关注对象的“具体内容/信息”，按 Key 与 Query 的匹配度加权提取。         | 各物品的具体信息：<br>- 书的 Value：书名《百年孤独》、作者马尔克斯、内容摘要<br>- 水杯的 Value：容量 500ml、水温 25℃<br>- 纸巾的 Value：品牌 XX、抽数 100 张 |\n",
    "\n",
    "\n",
    "### 2. 注意力计算实例（文本翻译任务）\n",
    "为简化理解，设定任务参数：\n",
    "- 模型维度 `d_model = 4`\n",
    "- 批次大小 `batch_size = 1`\n",
    "\n",
    "#### 核心矩阵形状定义\n",
    "- Query 形状：`(batch_size, seq_len_q, d_model)`（`seq_len_q` 为查询序列长度，此处对应中文句子长度）\n",
    "- Key 形状：`(batch_size, seq_len_k, d_model)`（`seq_len_k` 为键序列长度，此处对应英文句子长度）\n",
    "- Value 形状：`(batch_size, seq_len_k, d_model)`（与 Key 序列长度一致）\n",
    "\n",
    "\n",
    "#### 步骤1：计算注意力分数（未归一化）\n",
    "注意力分数用于衡量 Query 与 Key 的语义关联程度，计算公式为：  \n",
    "`scores = query @ key.transpose(-2, -1)`  \n",
    "\n",
    "- 矩阵乘法逻辑：`(1, 5, 4) @ (1, 4, 5) = (1, 5, 5)`（最终形状为 `(batch_size, seq_len_q, seq_len_k)`）\n",
    "- 数值含义：`scores[i][j]` 表示“中文第 i 个字”与“英文第 j 个词”的关联度，数值越高关联越强。\n",
    "\n",
    "以下为“中文句子（今、天、我、很、开心）→ 英文句子（I、am、very、happy、today）”的未归一化分数矩阵：\n",
    "\n",
    "| 中文Query（行）\\ 英文Key（列） | I    | am   | very | happy | today |\n",
    "|--------------------------------|------|------|------|-------|-------|\n",
    "| 今                             | 0.1  | 0.1  | 0.1  | 0.1   | **0.8**|\n",
    "| 天                             | 0.1  | 0.1  | 0.1  | 0.1   | **0.7**|\n",
    "| 我                             | **0.9**| 0.2  | 0.1  | 0.1   | 0.1   |\n",
    "| 很                             | 0.1  | 0.1  | **0.8**| 0.2   | 0.1   |\n",
    "| 开心                           | 0.1  | 0.2  | 0.3  | **0.9**| 0.1   |\n",
    "\n",
    "> 示例解读：中文“今”与英文“today”关联度最高（0.8），中文“开心”与英文“happy”关联度最高（0.9），符合语义逻辑。\n",
    "\n",
    "\n",
    "#### 步骤2：Softmax 归一化（转化为“关注度概率”）\n",
    "为让“关联度”转化为“可加权的概率”，对分数矩阵按行（`dim=-1`）做 Softmax 运算，确保每行数值和为 1。\n",
    "\n",
    "归一化后的注意力分数矩阵：\n",
    "\n",
    "| 中文Query（行）\\ 英文Key（列） | I    | am   | very | happy | today | 行和（验证） |\n",
    "|--------------------------------|------|------|------|-------|-------|--------------|\n",
    "| 今                             | 0.18 | 0.18 | 0.18 | 0.18  | **0.28**| 1.0          |\n",
    "| 天                             | 0.19 | 0.19 | 0.19 | 0.19  | **0.24**| 1.0          |\n",
    "| 我                             | **0.65**| 0.09 | 0.08 | 0.08   | 0.08   | 1.0          |\n",
    "| 很                             | 0.08 | 0.08 | **0.60**| 0.12   | 0.08   | 1.0          |\n",
    "| 开心                           | 0.08 | 0.09 | 0.11 | **0.62**| 0.10   | 1.0          |\n",
    "\n",
    "\n",
    "#### 步骤3：注意力加权（获取最终输出）\n",
    "用归一化后的分数矩阵对 Value 矩阵加权求和，公式为：  \n",
    "`out = scores @ value`  \n",
    "\n",
    "- 矩阵乘法逻辑：`(1, 5, 5) @ (1, 5, 4) = (1, 5, 4)`（最终形状与 Query 一致，为 `(batch_size, seq_len_q, d_model)`）\n",
    "- Value 矩阵定义：存储英文词的核心语义特征（此处设为4个维度：人称、情绪、程度、时间）。\n",
    "\n",
    "\n",
    "##### 英文词的 Value 矩阵\n",
    "| 英文词（Key/Value） | 维度0（人称） | 维度1（情绪） | 维度2（程度） | 维度3（时间） |\n",
    "|---------------------|---------------|---------------|---------------|---------------|\n",
    "| I                   | 0.9           | 0.2           | 0.1           | 0.1           |\n",
    "| am                  | 0.3           | 0.3           | 0.2           | 0.1           |\n",
    "| very                | 0.1           | 0.2           | 0.9           | 0.1           |\n",
    "| happy               | 0.2           | 0.9           | 0.3           | 0.1           |\n",
    "| today               | 0.1           | 0.1           | 0.1           | 0.9           |\n",
    "\n",
    "\n",
    "##### 注意力加权结果（out 矩阵）\n",
    "| 中文Query（行） | 维度0（人称）                                                                 | 维度1（情绪）                                                                 | 维度2（程度）                                                                 | 维度3（时间）                                                                 |\n",
    "|-----------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| 今              | 0.18×0.9 + 0.18×0.3 + 0.18×0.1 + 0.18×0.2 + 0.28×0.1 = 0.28                   | 0.18×0.2 + 0.18×0.3 + 0.18×0.2 + 0.18×0.9 + 0.28×0.1 = 0.25                   | 0.18×0.1 + 0.18×0.2 + 0.18×0.9 + 0.18×0.3 + 0.28×0.1 = 0.22                   | 0.18×0.1 + 0.18×0.1 + 0.18×0.1 + 0.18×0.1 + 0.28×0.9 = 0.30                   |\n",
    "| 天              | 0.19×0.9 + 0.19×0.3 + 0.19×0.1 + 0.19×0.2 + 0.24×0.1 = 0.27                   | 0.19×0.2 + 0.19×0.3 + 0.19×0.2 + 0.19×0.9 + 0.24×0.1 = 0.24                   | 0.19×0.1 + 0.19×0.2 + 0.19×0.9 + 0.19×0.3 + 0.24×0.1 = 0.21                   | 0.19×0.1 + 0.19×0.1 + 0.19×0.1 + 0.19×0.1 + 0.24×0.9 = 0.28                   |\n",
    "| 我              | 0.65×0.9 + 0.09×0.3 + 0.08×0.1 + 0.08×0.2 + 0.08×0.1 = 0.62                   | 0.65×0.2 + 0.09×0.3 + 0.08×0.2 + 0.08×0.9 + 0.08×0.1 = 0.22                   | 0.65×0.1 + 0.09×0.2 + 0.08×0.9 + 0.08×0.3 + 0.08×0.1 = 0.18                   | 0.65×0.1 + 0.09×0.1 + 0.08×0.1 + 0.08×0.1 + 0.08×0.9 = 0.16                   |\n",
    "| 很              | 0.08×0.9 + 0.08×0.3 + 0.60×0.1 + 0.12×0.2 + 0.08×0.1 = 0.18                   | 0.08×0.2 + 0.08×0.3 + 0.60×0.2 + 0.12×0.9 + 0.08×0.1 = 0.25                   | 0.08×0.1 + 0.08×0.2 + 0.60×0.9 + 0.12×0.3 + 0.08×0.1 = 0.59                   | 0.08×0.1 + 0.08×0.1 + 0.60×0.1 + 0.12×0.1 + 0.08×0.9 = 0.17                   |\n",
    "| 开心            | 0.08×0.9 + 0.09×0.3 + 0.11×0.1 + 0.62×0.2 + 0.10×0.1 = 0.22                   | 0.08×0.2 + 0.09×0.3 + 0.11×0.2 + 0.62×0.9 + 0.10×0.1 = 0.64                   | 0.08×0.1 + 0.09×0.2 + 0.11×0.9 + 0.62×0.3 + 0.10×0.1 = 0.32                   | 0.08×0.1 + 0.09×0.1 + 0.11×0.1 + 0.62×0.1 + 0.10×0.9 = 0.18                   |\n",
    "\n",
    "\n",
    "### 3. 结果\n",
    "#### 结果语义\n",
    "从加权输出矩阵可看出，中文每个字都精准捕捉到了英文对应词的核心特征：\n",
    "- 中文“今”“天”：维度3（时间）数值最高（0.30/0.28），对应英文“today”的时间特征；\n",
    "- 中文“我”：维度0（人称）数值最高（0.62），对应英文“I”的人称特征；\n",
    "- 中文“很”：维度2（程度）数值最高（0.59），对应英文“very”的程度特征；\n",
    "- 中文“开心”：维度1（情绪）数值最高（0.64），对应英文“happy”的情绪特征。\n",
    "\n",
    "\n",
    "#### 注意力机制总结\n",
    "1. **Value 存储特征**：Value 矩阵记录被关注对象（如英文词）的核心语义特征；\n",
    "2. **Score 建立关联**：注意力分数矩阵（经 Softmax 归一化）找到 Query（如中文词）与 Key（如英文词）的语义关联；\n",
    "3. **Weighted 定制输出**：通过分数加权 Value，为每个 Query 定制“最相关的语义特征”，后续解码器可基于此生成准确结果（如中文译文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ec421",
   "metadata": {},
   "source": [
    "## 多头注意力\n",
    "\n",
    "相比于基础注意力（单头注意力），多头注意力的实现流程有所差异，核心是通过“多组独立特征空间”捕捉更丰富的语义关联。\n",
    "\n",
    "\n",
    "### 1. 第一步：线性投影（Query/Key/Value 空间映射）\n",
    "对于输入的 Query、Key、Value，首先分别与三个独立的权重矩阵进行矩阵乘法（即线性投影），公式如下：  \n",
    "- $q = query \\ @ \\ W_q$  \n",
    "- $k = key \\ @ \\ W_k$  \n",
    "- $v = value \\ @ \\ W_v$  \n",
    "\n",
    "其中，三个权重矩阵的形状均为 $(d_{model}, d_{model})$，投影后 Query、Key、Value 的维度仍保持 $d_{model}$（与输入维度一致）。\n",
    "\n",
    "\n",
    "#### 为什么需要线性投影？\n",
    "基础注意力（单头）的局限性在于：Query、Key、Value 共享同一个 $d_{model}$ 维特征空间，所有语义信息（如人称、情绪、程度、时间等）都混合在这一个空间中，模型只能用“同一套标准”捕捉关联——类似用“万能工具”解决所有问题，无法对不同类型的信息做针对性处理。  \n",
    "\n",
    "而线性投影的核心作用，是为后续“拆分多特征空间”做准备：通过不同的权重矩阵 $W_q、W_k、W_v$，将原始 Query、Key、Value 映射到新的特征空间，后续再拆分为多个独立子空间，让每个子空间专注于捕捉某一类特定语义信息（例如：头1专注“人称+情绪”，头2专注“程度+时间”）。\n",
    "\n",
    "\n",
    "### 2. 第二步：拆分多头（构建多独立特征空间）\n",
    "线性投影后，需要将 Query、Key、Value 按“头数（heads）”拆分为多个子矩阵，具体步骤如下：\n",
    "\n",
    "#### 关键参数定义\n",
    "- 头数：$heads$（需满足 $d_{model} \\% heads = 0$，确保每个头的维度均匀）  \n",
    "- 每个头的维度：$head\\_dim = d_{model} // heads$  \n",
    "\n",
    "\n",
    "#### 矩阵形状变化\n",
    "以“$d_{model}=4$，$heads=2$”（延续前文翻译任务示例）为例，拆分前后的矩阵形状变化如下：  \n",
    "| 矩阵   | 线性投影后形状                | 拆分多头后形状                          |\n",
    "|--------|-----------------------------|---------------------------------------|\n",
    "| Query  | $(batch\\_size, seq\\_len_q, d_{model})$ | $(batch\\_size, seq\\_len_q, heads, head\\_dim)$ |\n",
    "| Key    | $(batch\\_size, seq\\_len_k, d_{model})$ | $(batch\\_size, seq\\_len_k, heads, head\\_dim)$ |\n",
    "| Value  | $(batch\\_size, seq\\_len_k, d_{model})$ | $(batch\\_size, seq\\_len_k, heads, head\\_dim)$ |\n",
    "\n",
    "代入具体数值（$batch\\_size=1$，$seq\\_len_q=5$，$seq\\_len_k=5$）：  \n",
    "- 投影后：$(1, 5, 4)$  \n",
    "- 拆分后：$(1, 5, 2, 2)$（2个头，每个头维度为2）  \n",
    "\n",
    "\n",
    "### 3. 第三步：调整形状与计算注意力分数\n",
    "为了让每个头独立计算注意力关联，需要先调整 Query 和 Key 的维度顺序，再进行矩阵乘法计算分数：\n",
    "\n",
    "#### 形状调整逻辑\n",
    "- Query 调整：从 $(batch\\_size, seq\\_len_q, heads, head\\_dim)$ 变为 $(batch\\_size, heads, seq\\_len_q, head\\_dim)$  \n",
    "  → 目的：将“头数（heads）”维度提前，确保每个头的计算相互独立。  \n",
    "- Key 调整：从 $(batch\\_size, seq\\_len_k, heads, head\\_dim)$ 变为 $(batch\\_size, heads, head\\_dim, seq\\_len_k)$  \n",
    "  → 目的：满足矩阵乘法维度要求（Query 的最后一维需与 Key 的倒数第二维一致）。  \n",
    "\n",
    "\n",
    "#### 计算注意力分数\n",
    "调整形状后，通过矩阵乘法计算每个头的注意力分数：  \n",
    "$scores = q \\ @ \\ k$  \n",
    "\n",
    "- 形状变化：$(batch\\_size, heads, seq\\_len_q, head\\_dim) \\ @ \\ (batch\\_size, heads, head\\_dim, seq\\_len_k) = (batch\\_size, heads, seq\\_len_q, seq\\_len_k)$  \n",
    "- 数值含义：$scores[b][h][i][j]$ 表示“第 b 个批次、第 h 个头”中，Query 的第 i 个元素与 Key 的第 j 个元素的关联度。\n",
    "\n",
    "\n",
    "### 4. 因果掩码（避免未来信息泄露）\n",
    "在文本翻译、文本生成等**自回归任务**中，模型需要“根据前一时刻的输出预测下一时刻结果”，因此必须确保：计算当前位置的注意力时，**不能看到后续位置的信息**（即“未来信息泄露”）。因果掩码（Causal Mask）就是解决这一问题的核心手段。\n",
    "\n",
    "\n",
    "#### 1）未加掩码的问题（未来信息泄露）\n",
    "以中文句子“今、天、我、很、开心”的自注意力计算为例（Query 和 Key 均为该句子），未加掩码时的注意力分数矩阵如下（行=Query 位置，列=Key 位置）：\n",
    "\n",
    "| 中文Query（行）\\ 中文Key（列） | 今          | 天          | 我          | 很          | 开心        |\n",
    "|--------------------------------|-------------|-------------|-------------|-------------|-------------|\n",
    "| 今                             | 今/今       | 今/天       | 今/我       | 今/很       | 今/开心     |\n",
    "| 天                             | 天/今       | 天/天       | 天/我       | 天/很       | 天/开心     |\n",
    "| 我                             | 我/今       | 我/天       | 我/我       | 我/很       | 我/开心     |\n",
    "| 很                             | 很/今       | 很/天       | 很/我       | 很/很       | 很/开心     |\n",
    "| 开心                           | 开心/今     | 开心/天     | 开心/我     | 开心/很     | 开心/开心   |\n",
    "\n",
    "此时存在严重问题：计算“今”的注意力时，模型能看到“天、我、很、开心”等未来位置的信息，违背自回归任务的逻辑（模型本应只基于“今”之前的信息预测后续）。\n",
    "\n",
    "\n",
    "#### 2）加因果掩码后的效果\n",
    "因果掩码的核心操作：将“Query 位置 < Key 位置”的元素（即矩阵右上角区域）设为 $-\\infty$（负无穷）。这样在后续 Softmax 归一化时，$-\\infty$ 会被映射为 0，模型将不再关注这些“未来位置”的信息。\n",
    "\n",
    "加掩码后的注意力分数矩阵如下：\n",
    "\n",
    "| 中文Query（行）\\ 中文Key（列） | 今          | 天          | 我          | 很          | 开心        |\n",
    "|--------------------------------|-------------|-------------|-------------|-------------|-------------|\n",
    "| 今                             | 今/今       | $-\\inf$     | $-\\inf$     | $-\\inf$     | $-\\inf$     |\n",
    "| 天                             | 天/今       | 天/天       | $-\\inf$     | $-\\inf$     | $-\\inf$     |\n",
    "| 我                             | 我/今       | 我/天       | 我/我       | $-\\inf$     | $-\\inf$     |\n",
    "| 很                             | 很/今       | 很/天       | 很/我       | 很/很       | $-\\inf$     |\n",
    "| 开心                           | 开心/今     | 开心/天     | 开心/我     | 开心/很     | 开心/开心   |\n",
    "\n",
    "此时每个 Query 位置仅能关注“当前及之前”的 Key 位置，完全符合自回归任务的逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15796",
   "metadata": {},
   "source": [
    "## 多头注意力类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05770992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.191955Z",
     "start_time": "2025-09-06T15:09:43.185710Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model: int, heads: int, dropout: float = 0, mask: bool = False\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model) # 变换回 d_model\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "\n",
    "        self.heads = heads\n",
    "\n",
    "        self.head_dim = d_model // heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        batch_size, seq_len_q = query.size(0), query.size(1)\n",
    "        seq_len_k = key.size(1)\n",
    "        # 线性投影,分割多头\n",
    "        # (batch_size,heads,seq_len_q,head_dim)\n",
    "        q = (\n",
    "            self.W_q(query)\n",
    "            .view(batch_size, seq_len_q, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        k = (\n",
    "            self.W_k(key)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        # (batch_size,heads,seq_len_k,head_dim)\n",
    "        v = (\n",
    "            self.W_v(value)\n",
    "            .view(batch_size, seq_len_k, self.heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # 计算注意力分数\n",
    "        # scores:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        scores = q @ k.transpose(-2, -1)\n",
    "        # 因果掩码,防止模型看见未来的信息\n",
    "        if self.mask:\n",
    "            mask_matrix = torch.triu(\n",
    "                torch.full((seq_len_q, seq_len_k), float(\"-inf\")), diagonal=1\n",
    "            ).to(query.device)\n",
    "            scores = scores + mask_matrix\n",
    "        # 掩蔽字符<pad>,因为它无意义\n",
    "        if key_padding_mask is not None:\n",
    "            # 确保key_padding_mask是布尔类型\n",
    "            if key_padding_mask.dtype != torch.bool:\n",
    "                key_padding_mask = key_padding_mask.bool()\n",
    "\n",
    "            # 原始形状: (batch_size, seq_len_k)\n",
    "            # 目标形状: (batch_size, 1, 1, seq_len_k) ,这样可以广播到所有头和query位置\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "            # 应用掩码\n",
    "            scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "        # 缩放并应用softmax\n",
    "        attention = nn.Softmax(dim=-1)(\n",
    "            scores / torch.sqrt(torch.tensor(self.head_dim, device=query.device))\n",
    "        )\n",
    "        # attention:(batch_size,heads,seq_len_q,seq_len_k)\n",
    "        attention = self.dropout(attention)\n",
    "        # 加权和\n",
    "        # out:(batch_size,heads,seq_len_q,head_dim)\n",
    "        out = attention @ v\n",
    "        # 拼接多头\n",
    "        out = (\n",
    "            out.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len_q, self.heads * self.head_dim)\n",
    "        )\n",
    "        # (batch_size,seq_len_q,d_model)\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdc6ec",
   "metadata": {},
   "source": [
    "# 编码器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9b551",
   "metadata": {},
   "source": [
    "到现在为止我们已经实现了Transformer最核心的多头注意力类,以下是编码器的详细结构(图示见论文Attention is all you need.pdf)<br>\n",
    "## 作用：负责“理解输入序列”\n",
    "编码器的核心功能是对输入序列（如翻译任务中的“英文句子”）进行语义编码，输出包含全局语义信息的特征矩阵。其结构具有 **“堆叠性”** 和 **“对称性”** ——通常由 N 个完全相同的“编码器层（Encoder Layer）”串联组成（原论文中 N=6）。\n",
    "\n",
    "\n",
    "### 1. 编码器整体结构（N 层堆叠）\n",
    "```\n",
    "[输入嵌入层 + 位置编码] → [编码器层 1] → [编码器层 2] → ... → [编码器层 N] → [编码器输出]\n",
    "```\n",
    "\n",
    "#### 各模块作用说明\n",
    "| 模块                | 输入形状                  | 输出形状                  | 核心作用                                                                 |\n",
    "|---------------------|--------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| 输入嵌入层          | (batch_size, seq_len_in) | (batch_size, seq_len_in, d_model) | 将输入序列的“整数索引”映射为高维向量（嵌入向量），捕捉基础语义。           |\n",
    "| 位置编码            | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 为嵌入向量添加“位置信息”（Transformer 无循环结构，需显式注入位置）。     |\n",
    "| 编码器层（N 个）    | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 核心语义处理单元，通过“自注意力+前馈网络”深化序列的全局语义关联。         |\n",
    "| 编码器输出          | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 输出包含输入序列全局语义的特征矩阵，供解码器的“编码器-解码器注意力”使用。 |\n",
    "\n",
    "\n",
    "### 2. 单个编码器层（Encoder Layer）结构\n",
    "每个编码器层内部遵循“注意力→残差连接→归一化→前馈网络→残差连接→归一化”的流程，具体结构如下：\n",
    "```\n",
    "[输入] → [多头自注意力（Multi-Head Self-Attention）] → [残差连接 + 层归一化（Layer Norm）] → \n",
    "[前馈神经网络（Feed-Forward Network）] → [残差连接 + 层归一化（Layer Norm）] → [输出]\n",
    "```\n",
    "\n",
    "#### 单个编码器层核心模块拆解\n",
    "| 模块                  | 输入形状                  | 输出形状                  | 核心逻辑                                                                 |\n",
    "|-----------------------|--------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| 多头自注意力          | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 让序列中每个位置“关注自身及其他所有位置”，捕捉全局语义关联（无因果掩码，因输入序列无需隐藏未来信息）。 |\n",
    "| 残差连接（Residual Connection） | 两个输入：<br>1. 模块原始输入<br>2. 模块输出 | (batch_size, seq_len_in, d_model) | 公式：`output = input + module_output`，缓解深度模型的梯度消失问题，保留原始输入信息。 |\n",
    "| 层归一化（Layer Norm） | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 对每个样本的特征维度做归一化（均值=0，方差=1），公式：`LayerNorm(x) = γ·(x-μ)/√(σ²+ε) + β`，加速训练收敛。 |\n",
    "| 前馈神经网络          | (batch_size, seq_len_in, d_model) | (batch_size, seq_len_in, d_model) | 对每个位置的特征向量独立做非线性变换，增强模型表达能力。结构：`Linear(d_model, d_ff) → ReLU → Linear(d_ff, d_model)`（原论文中 d_ff=2048）。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ae380",
   "metadata": {},
   "source": [
    "## 编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd3c377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.197148Z",
     "start_time": "2025-09-06T15:09:43.192961Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.multi_attention = MultiHeadAttention(\n",
    "            d_model=d_model, heads=heads, dropout=dropout, mask=False\n",
    "        )  # 自注意力，不需要因果掩码\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)  # 层归一化\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )  # 前馈网络\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, src_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # 自注意力块\n",
    "        # src:(batch_size,seq_len,d_model)\n",
    "        residual_1 = src.clone()\n",
    "        x = self.multi_attention(\n",
    "            src,\n",
    "            src,\n",
    "            src,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        x = self.layer_norm_1(x + residual_1)\n",
    "\n",
    "        # 前馈网络块\n",
    "        residual_2 = x.clone()  # 深拷贝\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.layer_norm_2(x + residual_2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb134355",
   "metadata": {},
   "source": [
    "# 解码器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bae77",
   "metadata": {},
   "source": [
    "编码器的图示架构见论文Attention is all you need.pdf<br>\n",
    "与编码器不同，解码器有两个多头注意力块，第一个是对解码器的输入进行自注意力，这一步需要因果掩码，第二个是将编码器的输出作为key,value,将解码器自注意力块的输出作为query的多头注意力<br>\n",
    "## 作用：负责“生成输出序列”\n",
    "解码器的核心功能是基于“编码器的语义输出”和“已生成的部分输出序列”，自回归生成完整的输出序列（如翻译任务中的“中文句子”）。与编码器类似，解码器也由 N 个完全相同的“解码器层（Decoder Layer）”串联组成（原论文中 N=6）。\n",
    "\n",
    "\n",
    "### 1. 解码器整体结构（N 层堆叠）\n",
    "```\n",
    "[目标嵌入层 + 位置编码] → [解码器层 1] → [解码器层 2] → ... → [解码器层 N] → [Linear + Softmax] → [最终输出序列]\n",
    "```\n",
    "\n",
    "#### 各模块作用说明\n",
    "| 模块                | 输入形状                  | 输出形状                  | 核心作用                                                                 |\n",
    "|---------------------|--------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| 目标嵌入层          | (batch_size, seq_len_out) | (batch_size, seq_len_out, d_model) | 将“已生成的部分输出序列”（整数索引）映射为高维嵌入向量，捕捉生成序列的基础语义。 |\n",
    "| 位置编码            | (batch_size, seq_len_out, d_model) | (batch_size, seq_len_out, d_model) | 为目标嵌入向量添加位置信息，确保生成序列的时序逻辑。                       |\n",
    "| 解码器层（N 个）    | 两个输入：<br>1. 目标嵌入+位置编码<br>2. 编码器输出 | (batch_size, seq_len_out, d_model) | 核心生成单元，通过“掩码自注意力+编码器-解码器注意力+前馈网络”实现精准生成。 |\n",
    "| Linear + Softmax    | (batch_size, seq_len_out, d_model) | (batch_size, seq_len_out, vocab_size) | Linear 将 d_model 维特征映射到词汇表维度，Softmax 转化为“每个词的生成概率”，选择概率最高的词作为当前输出。 |\n",
    "\n",
    "\n",
    "### 2. 单个解码器层（Decoder Layer）结构\n",
    "每个解码器层比编码器层多一个“编码器-解码器注意力”模块，用于关联“输入语义”和“生成序列”，具体流程如下：\n",
    "```\n",
    "[输入] → [掩码多头自注意力（Masked Multi-Head Self-Attention）] → [残差连接 + 层归一化] → \n",
    "[编码器-解码器注意力（Encoder-Decoder Attention）] → [残差连接 + 层归一化] → \n",
    "[前馈神经网络] → [残差连接 + 层归一化] → [输出]\n",
    "```\n",
    "\n",
    "#### 单个解码器层核心模块拆解\n",
    "| 模块                  | 输入形状                  | 输出形状                  | 核心逻辑                                                                 |\n",
    "|-----------------------|--------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| 掩码多头自注意力      | (batch_size, seq_len_out, d_model) | (batch_size, seq_len_out, d_model) | 对“已生成的部分输出序列”做自注意力，**添加因果掩码**（仅允许关注“当前及之前位置”），避免未来信息泄露（自回归生成的关键）。 |\n",
    "| 编码器-解码器注意力  | 两个输入：<br>1. 掩码自注意力输出<br>2. 编码器输出 | (batch_size, seq_len_out, d_model) | 让“生成序列的每个位置”关注“输入序列的所有位置”，实现“输入语义”与“生成内容”的关联（如翻译时，中文“今”关注英文“today”）。 |\n",
    "| 前馈神经网络          | (batch_size, seq_len_out, d_model) | (batch_size, seq_len_out, d_model) | 与编码器的前馈网络结构完全一致，对每个位置的特征向量独立做非线性变换，增强生成能力。 |\n",
    "| 残差连接 + 层归一化  | 同编码器层                | 同编码器层                | 作用与编码器层一致：缓解梯度消失、加速收敛、保留原始信息。               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9281e96",
   "metadata": {},
   "source": [
    "## 解码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f429c3af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.214891Z",
     "start_time": "2025-09-06T15:09:43.208885Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "        self.multi_attention_1 = MultiHeadAttention(\n",
    "            d_model=d_model, heads=heads, dropout=dropout, mask=True\n",
    "        )\n",
    "        self.multi_attention_2 = MultiHeadAttention(\n",
    "            d_model=d_model, heads=heads, dropout=dropout, mask=False\n",
    "        )\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_out: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # eocoder_out:(batch_size,seq_len_q,d_model)\n",
    "        # tgt:(batch_size,seq_len_k,d_model)\n",
    "        # 自注意力块（带掩码）\n",
    "        residual_1 = tgt.clone()\n",
    "        # (batch_size,seq_len_k,d_model)\n",
    "        x = self.multi_attention_1(\n",
    "            tgt,\n",
    "            tgt,\n",
    "            tgt,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        x = self.layer_norm_1(x + residual_1)\n",
    "\n",
    "        # 交叉注意力块（编码器-解码器注意力）\n",
    "        residual_2 = x.clone()\n",
    "        x = self.multi_attention_2(\n",
    "            query=x, # 注意解码器自注意力输出为查询\n",
    "            key=encoder_out,\n",
    "            value=encoder_out,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        x = self.layer_norm_2(x + residual_2)\n",
    "\n",
    "        # 前馈网络块\n",
    "        residual_3 = x.clone()\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.layer_norm_3(x + residual_3)\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b17da5",
   "metadata": {},
   "source": [
    "# 完整模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfb6e5",
   "metadata": {},
   "source": [
    "到目前为止我们实现了Transformer中的所有组件，我们可以将它们组装为完整的Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8251231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:09:43.236363Z",
     "start_time": "2025-09-06T15:09:43.230692Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        heads: int = 4,\n",
    "        d_model: int = 512,\n",
    "        num_layers: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.head_dim = d_model // heads\n",
    "        self.encoder_embedding_position = EmbeddingPositionEncode(\n",
    "            d_model=d_model, dropout=dropout, vocab_size=src_vocab_size\n",
    "        )\n",
    "        self.decoder_embedding_position = EmbeddingPositionEncode(\n",
    "            d_model=d_model, dropout=dropout, vocab_size=tgt_vocab_size\n",
    "        )\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(d_model=d_model, heads=heads, dropout=dropout)\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.decoders = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(d_model=d_model, heads=heads, dropout=dropout)\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # 权重初始化\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - src :(batch_size,seq_len)\n",
    "        - tgt:(batch_size,seq_len)\n",
    "        \"\"\"\n",
    "        # 嵌入并添加位置编码\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        src_embedded = self.encoder_embedding_position(src)\n",
    "        tgt_embedded = self.decoder_embedding_position(tgt)\n",
    "\n",
    "        # 编码器\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        encoder_out = src_embedded\n",
    "        for encode_layer in self.encoders:\n",
    "            encoder_out = encode_layer(encoder_out, src_key_padding_mask)\n",
    "        # 解码器\n",
    "        # (batch_size,seq_len,d_model)\n",
    "        decoder_out = tgt_embedded\n",
    "        for decoder_layer in self.decoders:\n",
    "            decoder_out = decoder_layer(\n",
    "                encoder_out, decoder_out, src_key_padding_mask, tgt_key_padding_mask\n",
    "            )\n",
    "\n",
    "        # 最终线性层\n",
    "        x = self.final_linear(decoder_out)\n",
    "        # (batch_size,seq_len,vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53cc46",
   "metadata": {},
   "source": [
    "# 训练\n",
    "为了检验我们模型是否真的有效，我们先在10个样本上面进行过拟合训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1b9c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:10:07.426711Z",
     "start_time": "2025-09-06T15:09:43.261233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps: 400,warmup_steps: 160\n",
      "epoch 10, loss: 4.843435, perplexity: 126.904556,val_loss: 5.048058032989502\n",
      "epoch 20, loss: 4.652657, perplexity: 104.863243,val_loss: 4.995823383331299\n",
      "epoch 30, loss: 4.449622, perplexity: 85.594559,val_loss: 5.173519134521484\n",
      "epoch 40, loss: 4.210688, perplexity: 67.402870,val_loss: 5.257781505584717\n",
      "epoch 50, loss: 3.811087, perplexity: 45.199551,val_loss: 5.317668437957764\n",
      "epoch 60, loss: 3.330725, perplexity: 27.958597,val_loss: 5.601557731628418\n",
      "epoch 70, loss: 2.825965, perplexity: 16.877222,val_loss: 5.946801662445068\n",
      "epoch 80, loss: 2.335989, perplexity: 10.339684,val_loss: 6.281468868255615\n",
      "epoch 90, loss: 1.792135, perplexity: 6.002252,val_loss: 6.638423919677734\n",
      "epoch 100, loss: 1.278914, perplexity: 3.592736,val_loss: 7.004504203796387\n",
      "epoch 110, loss: 0.763461, perplexity: 2.145690,val_loss: 7.9070281982421875\n",
      "epoch 120, loss: 0.269097, perplexity: 1.308782,val_loss: 8.102046966552734\n",
      "epoch 130, loss: 0.092729, perplexity: 1.097164,val_loss: 8.756143569946289\n",
      "epoch 140, loss: 0.039876, perplexity: 1.040682,val_loss: 9.062393188476562\n",
      "epoch 150, loss: 0.022672, perplexity: 1.022931,val_loss: 9.357833862304688\n",
      "epoch 160, loss: 0.013647, perplexity: 1.013740,val_loss: 9.443130493164062\n",
      "epoch 170, loss: 0.008125, perplexity: 1.008159,val_loss: 9.631153106689453\n",
      "epoch 180, loss: 0.006172, perplexity: 1.006191,val_loss: 9.786392211914062\n",
      "epoch 190, loss: 0.005079, perplexity: 1.005092,val_loss: 9.840686798095703\n",
      "epoch 200, loss: 0.004338, perplexity: 1.004347,val_loss: 9.957571029663086\n",
      "epoch 210, loss: 0.003789, perplexity: 1.003797,val_loss: 10.038956642150879\n",
      "epoch 220, loss: 0.003354, perplexity: 1.003360,val_loss: 10.109600067138672\n",
      "epoch 230, loss: 0.002993, perplexity: 1.002998,val_loss: 10.177303314208984\n",
      "epoch 240, loss: 0.002686, perplexity: 1.002690,val_loss: 10.246492385864258\n",
      "epoch 250, loss: 0.002420, perplexity: 1.002423,val_loss: 10.317514419555664\n",
      "epoch 260, loss: 0.002186, perplexity: 1.002188,val_loss: 10.386821746826172\n",
      "epoch 270, loss: 0.001979, perplexity: 1.001981,val_loss: 10.452251434326172\n",
      "epoch 280, loss: 0.001795, perplexity: 1.001797,val_loss: 10.520124435424805\n",
      "epoch 290, loss: 0.001631, perplexity: 1.001632,val_loss: 10.583711624145508\n",
      "epoch 300, loss: 0.001483, perplexity: 1.001484,val_loss: 10.651596069335938\n",
      "epoch 310, loss: 0.001350, perplexity: 1.001351,val_loss: 10.710814476013184\n",
      "epoch 320, loss: 0.001229, perplexity: 1.001230,val_loss: 10.772865295410156\n",
      "epoch 330, loss: 0.001121, perplexity: 1.001121,val_loss: 10.832648277282715\n",
      "epoch 340, loss: 0.001022, perplexity: 1.001022,val_loss: 10.892595291137695\n",
      "epoch 350, loss: 0.000932, perplexity: 1.000933,val_loss: 10.949994087219238\n",
      "epoch 360, loss: 0.000851, perplexity: 1.000851,val_loss: 11.0096435546875\n",
      "epoch 370, loss: 0.000776, perplexity: 1.000777,val_loss: 11.068222045898438\n",
      "epoch 380, loss: 0.000709, perplexity: 1.000709,val_loss: 11.126166343688965\n",
      "epoch 390, loss: 0.000647, perplexity: 1.000647,val_loss: 11.183248519897461\n",
      "epoch 400, loss: 0.000591, perplexity: 1.000591,val_loss: 11.240667343139648\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from TranslationDataset import TranslationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    file_path=\"../data/translate/TranslationData.csv\",\n",
    "    max_lines=10,  # 读取10行\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])  # 百分之九十作为训练集\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "src_vocab_size = len(dataset.ch_token_to_index)\n",
    "tgt_vocab_size = len(dataset.en_token_to_index)\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=64,\n",
    "    num_layers=6,\n",
    "    heads=4,\n",
    "    dropout=0,  # 这里我们因为是过拟合，所以dropout设置为0\n",
    ").to(\n",
    "    device\n",
    ")  # 如果你要拟合大数据集,d_moel,num_layers,heads,dropout需要做出相应的调整\n",
    "\n",
    "epochs = 400\n",
    "lr = 1.0  # 因为使用学习率调度器，因此我们初始学习率可以比较大\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,  # weight_decay=1e-4 # 同样,为了过拟合不使用权重衰减\n",
    ")\n",
    "padding_idx = dataset.en_token_to_index[\n",
    "    \"<pad>\"\n",
    "]  # 忽略填充字符的损失计算,因为它没有实际意义\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "batch_size = 10\n",
    "\n",
    "train_loader, test_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size\n",
    "), DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "total_steps = int(epochs * (len(dataset) / batch_size))\n",
    "warmup_steps = int(total_steps * 0.4)  # 我们设置预热步骤为总步数的百分之40\n",
    "print(f\"total_steps: {total_steps},warmup_steps: {warmup_steps}\")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda step: (\n",
    "        512 ** (-0.5)  # 模型维度的平方根倒数\n",
    "        * min(\n",
    "            (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
    "            (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval().to(device)\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for src, tgt in test_loader:\n",
    "            tgt_input = tgt[:, :-1]  # (batch_size, seq_len-1)\n",
    "            # 预测目标去掉第一个<bos>\n",
    "            tgt_target = tgt[:, 1:]  # (batch_size, seq_len-1)\n",
    "\n",
    "            src_key_padding_mask = (src == dataset.ch_token_to_index[\"<pad>\"]).to(\n",
    "                device\n",
    "            )\n",
    "            tgt_key_padding_mask = (tgt_input == dataset.en_token_to_index[\"<pad>\"]).to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "            src, tgt_input, tgt_target = (\n",
    "                src.to(device),\n",
    "                tgt_input.to(device),\n",
    "                tgt_target.to(device),\n",
    "            )\n",
    "            pred = model(src, tgt_input, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "            loss = loss_fn(pred.reshape(-1, tgt_vocab_size), tgt_target.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(test_loader)\n",
    "\n",
    "\n",
    "error = []\n",
    "path = None  # 如果你有保存好的检查点，在这里填充具体的文件路径\n",
    "start_epoch = 0\n",
    "\n",
    "# start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path) # 如果你有保存好的模型，那么取消这一行的注释\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()  # 开启训练模式\n",
    "    total_loss = 0\n",
    "    best_val_loss = 1e10  # 设置初始最佳loss\n",
    "    for src, tgt in train_loader:\n",
    "        # 解码器输入取tgt的前n-1个token（去掉最后一个<eos>或<pad>）\n",
    "        tgt_input = tgt[:, :-1]  # 形状：(batch_size, seq_len-1)\n",
    "        # 预测目标取tgt的后n-1个token（去掉第一个<bos>）\n",
    "        tgt_target = tgt[:, 1:]  # 形状：(batch_size, seq_len-1)\n",
    "\n",
    "        # 生成掩码（基于tgt_input的长度）\n",
    "        src_key_padding_mask = (src == dataset.ch_token_to_index[\"<pad>\"]).to(device)\n",
    "        tgt_key_padding_mask = (tgt_input == dataset.en_token_to_index[\"<pad>\"]).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # 移到设备\n",
    "        src, tgt_input, tgt_target = (\n",
    "            src.to(device),\n",
    "            tgt_input.to(device),\n",
    "            tgt_target.to(device),\n",
    "        )\n",
    "\n",
    "        # 模型前向传播（用tgt_input而非完整tgt）\n",
    "        pred = model(src, tgt_input, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "        # 计算损失（pred的形状是(batch, seq_len-1, tgt_vocab_size)，与tgt_target对齐）\n",
    "        loss = loss_fn(pred.reshape(-1, tgt_vocab_size), tgt_target.reshape(-1))\n",
    "\n",
    "        # 参数更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=5.0\n",
    "        )  # 设置5为最大梯度\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    val_loss = evaluate(model, test_loader, device)  # 训练集上评估\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # 保存最佳模型\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(\n",
    "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f},val_loss: {val_loss}\"\n",
    "        )\n",
    "    # 如果你需要保存检查点，取消下面的注释并更改path_to_save为具体的文件路径\n",
    "    # if (epoch + 1) % 50 == 0: #\n",
    "    #     save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path)\n",
    "    #     path_to_save = None\n",
    "    #     print(f\"已保存为{path_to_save}\")\n",
    "    error.append(loss.item())\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3f096",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8577da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T15:13:33.808204Z",
     "start_time": "2025-09-06T15:13:33.768200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文输入: 大会 1992 年 12 月 22 日 第 47 / 180 号 决议 决定 召开 联合国 人类 住 区 会议 ( 生境 二 ) ， 由 尽可能 高级 别的 代表 参与 。 \n",
      "英文翻译:  in resolution 47 / 180 of 22 December 1992 , the General Assembly decided to convene the United Nations Conference on Human Settlements at the highest possible level of participation . \n"
     ]
    }
   ],
   "source": [
    "def preprocess_chinese(sentence, ch_token_to_index):\n",
    "    # 分词\n",
    "    tokens = sentence.split()\n",
    "    # 转换为索引，未知词用<unk>\n",
    "    index_tokens = [\n",
    "        ch_token_to_index.get(token, ch_token_to_index[\"<unk>\"]) for token in tokens\n",
    "    ]\n",
    "    # 添加首尾标记\n",
    "    index_tokens = (\n",
    "        [ch_token_to_index[\"<bos>\"]] + index_tokens + [ch_token_to_index[\"<eos>\"]]\n",
    "    )\n",
    "   \n",
    "    # 增加batch维度\n",
    "    return torch.tensor(index_tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "def translate(chinese_sentence, model, dataset, device):\n",
    "    model.eval().to(device)\n",
    "    ch_token_to_index = dataset.ch_token_to_index\n",
    "    en_token_to_index = dataset.en_token_to_index\n",
    "    en_index_to_token = dataset.en_index_to_token\n",
    "\n",
    "    # 预处理中文输入\n",
    "    src_tensor = preprocess_chinese(\n",
    "        chinese_sentence,\n",
    "        ch_token_to_index, \n",
    "    ).to(device)\n",
    "    src_key_padding_mask = (src_tensor == ch_token_to_index[\"<pad>\"]).to(device)\n",
    "\n",
    "    # 获取编码器输出（只需计算一次）\n",
    "    # 如果我们直接调用model.forward方法，那么自回归生成中每一次都要计算一次解码器输出，因此我们在预测函数中每个层我们单独使用\n",
    "    with torch.no_grad():\n",
    "        src_embedded = model.encoder_embedding_position(src_tensor)\n",
    "        encoder_out = src_embedded\n",
    "        for encode_layer in model.encoders:\n",
    "            encoder_out = encode_layer(encoder_out, src_key_padding_mask)\n",
    "\n",
    "    # 自回归生成英文翻译\n",
    "    tgt_tokens = [en_token_to_index[\"<bos>\"]]  # 从起始标记开始\n",
    "    tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(dataset.en_max_length - 1):  # 限制最大长度\n",
    "        # 目标序列的填充掩码\n",
    "        tgt_key_padding_mask = (tgt_tensor == en_token_to_index[\"<pad>\"]).to(device)\n",
    "\n",
    "        # 解码器前向传播\n",
    "        with torch.no_grad():\n",
    "            tgt_embedded = model.decoder_embedding_position(tgt_tensor)\n",
    "            decoder_out = tgt_embedded\n",
    "            for decoder_layer in model.decoders:\n",
    "                decoder_out = decoder_layer(\n",
    "                    encoder_out, decoder_out, src_key_padding_mask, tgt_key_padding_mask\n",
    "                )\n",
    "            # 预测下一个token\n",
    "            pred_logits = model.final_linear(decoder_out)  # (1, seq_len, vocab_size)\n",
    "            next_token_idx = torch.argmax(pred_logits[:, -1, :], dim=-1).item()\n",
    "\n",
    "        # 添加到结果并更新输入\n",
    "        tgt_tokens.append(next_token_idx)\n",
    "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # 停止\n",
    "        if next_token_idx == en_token_to_index[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # 转换为英文句子\n",
    "    return (\n",
    "        \" \".join([en_index_to_token[idx] for idx in tgt_tokens])\n",
    "        .strip(\"<bos>\")\n",
    "        .strip(\"<eos>\")\n",
    "    )\n",
    "\n",
    "\n",
    "chinese_input = \"大会 1992 年 12 月 22 日 第 47 / 180 号 决议 决定 召开 联合国 人类 住 区 会议 ( 生境 二 ) ， 由 尽可能 高级 别的 代表 参与 。 \"\n",
    "translated_result = translate(chinese_input, model, dataset, device)\n",
    "print(\"中文输入:\", chinese_input)\n",
    "print(\"英文翻译:\", translated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfe1e0",
   "metadata": {},
   "source": [
    "# 模型泛化\n",
    "\n",
    "过拟合测试完成后，我们可以正式开始模型的训练<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\n",
    "    file_path=\"../data/translate/2m_WMT21.csv\",\n",
    "    max_lines=1500000,# 大样本\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "src_vocab_size = len(dataset.ch_token_to_index)\n",
    "tgt_vocab_size = len(dataset.en_token_to_index)\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    heads=8,\n",
    "    dropout=0.3,\n",
    ")\n",
    "# 训练步骤不变,训练10-15个epoch即可\n",
    "# 训练步骤...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbef996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若加载训练好的检查点，取消注释\n",
    "# model.load_state_dict(torch.load(\"model_epoch_1.pth\")[\"model_state_dict\"])\n",
    "# chinese_input = \"今 天 我 很 开心 。\"\n",
    "# translate(chinese_input, model, dataset, \"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
