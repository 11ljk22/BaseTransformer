{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021ffb4",
   "metadata": {},
   "source": [
    "nn.Transformer只包含编码器和解码器，因此位置编码和嵌入以及最终线性层依然需要我们自己实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31347406",
   "metadata": {},
   "source": [
    "# 位置嵌入与编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPositionEncode(nn.Module):\n",
    "    def __init__(self, d_model, dropout: float, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # x_1:(batch_size,seq_len,d_model)\n",
    "        x_1 = self.embedding(input_tensor)\n",
    "        seq_len = input_tensor.size(1)\n",
    "\n",
    "        # 创建位置编码(正余弦)\n",
    "        position = torch.arange(seq_len, device=input_tensor.device).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, device=input_tensor.device)\n",
    "            * (\n",
    "                    -torch.log(torch.tensor(10000.0, device=input_tensor.device))\n",
    "                    / self.d_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pos_encoding = torch.zeros(seq_len, self.d_model, device=input_tensor.device)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加位置编码\n",
    "        x_2 = pos_encoding.unsqueeze(0)\n",
    "        return self.dropout(x_1 + x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba7729",
   "metadata": {},
   "source": [
    "# 完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChEnTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_embed_encoder_layer = EmbeddingPositionEncode(\n",
    "            d_model, dropout, src_vocab_size\n",
    "        )\n",
    "        self.pos_embed_decoder_layer = EmbeddingPositionEncode(\n",
    "            d_model, dropout, tgt_vocab_size\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        src_processed = self.pos_embed_encoder_layer(src)\n",
    "        tgt_processed = self.pos_embed_decoder_layer(tgt)\n",
    "        out = self.transformer(\n",
    "            src=src_processed,\n",
    "            tgt=tgt_processed,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True,\n",
    "        )\n",
    "\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7106f",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c58301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from TranslationDataset import TranslationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch ,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scheduler_type\": type(scheduler).__name__,\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(path)\n",
    "        if model:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            print(f\"从{checkpoint['epoch']}开始训练\")\n",
    "        if scheduler:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "    print(\"未发现检查点\")\n",
    "    return 0, float(\"inf\")\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(\n",
    "    file_path=\"../data/translate/TranslationData.csv\",\n",
    "    max_lines=10,\n",
    ")\n",
    "device = \"cuda\"\n",
    "src_vocab_size = len(dataset.ch_token_to_index)\n",
    "tgt_vocab_size = len(dataset.en_token_to_index)\n",
    "model = ChEnTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=64,\n",
    "    dropout=0,\n",
    ").to(device)\n",
    "\n",
    "epochs = 150\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,  # weight_decay=1e-4\n",
    ")\n",
    "padding_idx = dataset.en_token_to_index[\"<pad>\"]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "total_steps = int(epochs * (len(dataset) / batch_size))\n",
    "warmup_steps = int(total_steps * 0.4)\n",
    "print(f\"total_steps: {total_steps},warmup_steps: {warmup_steps}\")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer,\n",
    "    lr_lambda=lambda step: (\n",
    "        512 ** (-0.5)  # 模型维度的平方根倒数\n",
    "        * min(\n",
    "            (step + 1) ** (-0.5),  # 衰减阶段：步长的平方根倒数\n",
    "            (step + 1) * (warmup_steps ** (-1.5)),  # 预热阶段：线性增长\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "error = []\n",
    "path = None  # 如果你有保存好的模型，在这里填充具体的文件路径\n",
    "start_epoch = 0\n",
    "# start_epoch, loss = load_checkpoint(model, optimizer, scheduler, path)\n",
    "model.train()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        # 处理解码器输入和目标\n",
    "        tgt_input = tgt[:, :-1]  # 解码器输入：去掉最后一个token（<eos>/<pad>）\n",
    "        tgt_target = tgt[:, 1:]  # 预测目标：去掉第一个token（<bos>）\n",
    "\n",
    "        # 生成填充掩码\n",
    "        src_key_padding_mask = (src == dataset.ch_token_to_index[\"<pad>\"]).to(device)\n",
    "        tgt_key_padding_mask = (tgt_input == dataset.en_token_to_index[\"<pad>\"]).to(\n",
    "            device\n",
    "        )\n",
    "        # 生成因果掩码\n",
    "        # 用nn.Transformer的内置方法生成正方形后续掩码\n",
    "        tgt_input_seq_len = tgt_input.shape[1]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input_seq_len).to(\n",
    "            device\n",
    "        )\n",
    "        src, tgt_input, tgt_target = (\n",
    "            src.to(device),\n",
    "            tgt_input.to(device),\n",
    "            tgt_target.to(device),\n",
    "        )\n",
    "\n",
    "        # 与我们自己实现的模型不同，这里删除tgt_mask参数，依赖tgt_is_causal=True自动生成\n",
    "        pred = model(\n",
    "            src=src,\n",
    "            tgt=tgt_input,\n",
    "            tgt_mask=tgt_mask,  # 无需手动传掩码\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        # 计算损失和优化\n",
    "        loss = loss_fn(pred.reshape(-1, tgt_vocab_size), tgt_target.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(\n",
    "            f\"epoch {epoch + 1}, loss: {avg_loss:.6f}, perplexity: {torch.exp(torch.tensor(avg_loss)).item():.6f}\"\n",
    "        )\n",
    "    # 如果你需要保存模型，取消下面的注释并更改path_to_save为具体的文件路径\n",
    "    # if (epoch + 1) % 50 == 0: #\n",
    "    #     save_checkpoint(epoch + 1, model, optimizer, scheduler, loss, path)\n",
    "    #     path_to_save = None\n",
    "    #     print(f\"已保存为{path}\")\n",
    "    error.append(loss.item())\n",
    "plt.plot(error)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src, tgt = next(iter(dataloader))\n",
    "    tgt_input_eval = tgt[:, :-1]  # 解码器输入：去掉最后一个token\n",
    "    tgt_input_seq_len = tgt_input_eval.shape[1]\n",
    "\n",
    "    # 生成因果掩码\n",
    "    tgt_mask_eval = nn.Transformer.generate_square_subsequent_mask(tgt_input_seq_len).to(device)\n",
    "\n",
    "    # 生成填充掩码\n",
    "    src_key_padding_mask = (src == dataset.ch_token_to_index[\"<pad>\"]).to(device)\n",
    "    tgt_key_padding_mask = (tgt_input_eval == dataset.en_token_to_index[\"<pad>\"]).to(device)\n",
    "\n",
    "    # 移到对应设备\n",
    "    src, tgt_input_eval, tgt = src.to(device), tgt_input_eval.to(device), tgt.to(device)\n",
    "\n",
    "    # 前向传播\n",
    "    pred = model(\n",
    "        src=src,\n",
    "        tgt=tgt_input_eval,\n",
    "        tgt_mask=tgt_mask_eval,\n",
    "        src_key_padding_mask=src_key_padding_mask,\n",
    "        tgt_key_padding_mask=tgt_key_padding_mask\n",
    "    )\n",
    "\n",
    "    pred_tokens = pred.argmax(dim=-1)  # 形状：(batch_size, tgt_input_seq_len)\n",
    "    # 生成与pred_tokens同batch_size的eos_token，形状：(batch_size, 1)\n",
    "    eos_token = torch.full(\n",
    "        (pred_tokens.size(0), 1),  # 确保batch维度一致\n",
    "        dataset.en_token_to_index[\"<eos>\"],\n",
    "        device=device,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    # dim=1表示在序列长度维度拼接\n",
    "    pred_tokens = torch.cat([pred_tokens, eos_token], dim=1)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"预测:\", [dataset.en_index_to_token[idx] for idx in pred_tokens[0].cpu().numpy()])\n",
    "    print(\"真实:\", [dataset.en_index_to_token[idx] for idx in tgt[0].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da200e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chinese(sentence, ch_token_to_index, max_length):\n",
    "    \"\"\"预处理中文句子，转换为张量（保持不变）\"\"\"\n",
    "    tokens = sentence.split()\n",
    "    index_tokens = [ch_token_to_index.get(token, ch_token_to_index[\"<unk>\"]) for token in tokens]\n",
    "    index_tokens = [ch_token_to_index[\"<bos>\"]] + index_tokens + [ch_token_to_index[\"<eos>\"]]\n",
    "    if len(index_tokens) < max_length:\n",
    "        index_tokens += [ch_token_to_index[\"<pad>\"]] * (max_length - len(index_tokens))\n",
    "    else:\n",
    "        index_tokens = index_tokens[:max_length]\n",
    "    return torch.tensor(index_tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "def translate(chinese_sentence, model, dataset, device):\n",
    "    \"\"\"\n",
    "    适配PyTorch官方Transformer的翻译函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ch_token_to_index = dataset.ch_token_to_index\n",
    "    en_token_to_index = dataset.en_token_to_index\n",
    "    en_index_to_token = dataset.en_index_to_token\n",
    "    \n",
    "    # 预处理中文输入\n",
    "    src_tensor = preprocess_chinese(\n",
    "        chinese_sentence, \n",
    "        ch_token_to_index, \n",
    "        dataset.ch_max_length\n",
    "    ).to(device)\n",
    "    src_key_padding_mask = (src_tensor == ch_token_to_index[\"<pad>\"]).to(device)\n",
    "    \n",
    "    # 获取编码器输出\n",
    "    with torch.no_grad():\n",
    "        # 先进行嵌入和位置编码\n",
    "        src_processed = model.pos_embed_encoder_layer(src_tensor)\n",
    "        # 调用官方编码器\n",
    "        encoder_out = model.transformer.encoder(\n",
    "            src=src_processed,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            is_causal=False  # 编码器不需要因果掩码\n",
    "        )\n",
    "    \n",
    "    # 自回归生成英文翻译\n",
    "    tgt_tokens = [en_token_to_index[\"<bos>\"]]  # 从起始标记开始\n",
    "    tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(dataset.en_max_length - 1):\n",
    "        # 生成当前目标序列的因果掩码\n",
    "        tgt_seq_len = tgt_tensor.shape[1]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
    "        \n",
    "        # 生成目标序列的填充掩码\n",
    "        tgt_key_padding_mask = (tgt_tensor == en_token_to_index[\"<pad>\"]).to(device)\n",
    "        \n",
    "        # 解码器前向传播\n",
    "        with torch.no_grad():\n",
    "            # 目标序列嵌入和位置编码\n",
    "            tgt_processed = model.pos_embed_decoder_layer(tgt_tensor)\n",
    "            # 调用官方解码器\n",
    "            decoder_out = model.transformer.decoder(\n",
    "                tgt=tgt_processed,\n",
    "                memory=encoder_out,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_key_padding_mask=src_key_padding_mask,  # 复用源序列的填充掩码\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                tgt_is_causal=True,  # 明确因果关系\n",
    "                memory_is_causal=False\n",
    "            )\n",
    "\n",
    "            pred_logits = model.linear(decoder_out)\n",
    "            # 取最后一个位置的预测结果\n",
    "            next_token_idx = torch.argmax(pred_logits[:, -1, :], dim=-1).item()\n",
    "        \n",
    "        # 更新目标序列\n",
    "        tgt_tokens.append(next_token_idx)\n",
    "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 停止\n",
    "        if next_token_idx == en_token_to_index[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    return [en_index_to_token[idx] for idx in tgt_tokens]\n",
    "\n",
    "\n",
    "\n",
    "chinese_input = '缔约国 根据 《 任择 议定书 》 第 12 条 第 1 款 提交 的 初次 报告 的 准则 由 委员会 在 2002 年 2 月 1 日 第 777 次 会议 上 通过 。'\n",
    "\n",
    "# 执行翻译\n",
    "translated_result = translate(chinese_input, model, dataset, device)\n",
    "\n",
    "# 输出结果\n",
    "print(\"中文输入:\", chinese_input)\n",
    "print(\"英文翻译:\", ' '.join(translated_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28aba0",
   "metadata": {},
   "source": [
    "# 模型泛化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ba02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(\n",
    "    file_path=\"../data/translate/2m_WMT21.csv\",\n",
    "    max_lines=1500000,# 大样本\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "src_vocab_size = len(dataset.ch_token_to_index)\n",
    "tgt_vocab_size = len(dataset.en_token_to_index)\n",
    "model = ChEnTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "# 同样地，训练步骤不变,训练10-15个epoch即可\n",
    "# 训练步骤...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
